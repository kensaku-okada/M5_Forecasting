{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://www.kaggle.com/kyakovlev/m5-lags-features from https://www.kaggle.com/ejunichi/m5-three-shades-of-dark-darker-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import gc\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "import warnings\n",
    "\n",
    "# custom import\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from multiprocessing import Pool        # Multiprocess Runs\n",
    "\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constant variables for helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_CORES: 16\n"
     ]
    }
   ],
   "source": [
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "print(f\"N_CORES: {N_CORES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function nicely diplaying a head of Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "def display(*dfs, head=True):\n",
    "    for df in dfs:\n",
    "        IPython.display.display(df.head() if head else df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function fixing random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    \"\"\"Sets seed to make all processes deterministic     # type: int\n",
    "    \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function processing df in multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_df_in_multiprocess(func, t_split):\n",
    "    \"\"\"Process ds in Multiprocess\n",
    "    \n",
    "    \"\"\"\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"メモリ使用量を確認するためのシンプルな「メモリプロファイラ」\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "\n",
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    \"\"\"\n",
    "    dtypesを失わないための連結による結合\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  constant variables for data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DATA_DIR = os.path.sep.join([\"data\", \"M5_Three_shades_of_Dark_Darker_magic\"])\n",
    "\n",
    "BASE = \"clearned_base_grid_for_darker_magic.pkl\"\n",
    "PRICE = \"base_grid_with_sales_price_features_for_darker_magic.pkl\"\n",
    "CALENDAR = \"base_grid_with_calendar_features_for_darker_magic.pkl\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    reduce the memory usage of the given dataframe.\n",
    "    https://qiita.com/hiroyuki_kageyama/items/02865616811022f79754\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe\n",
    "        verbose: \n",
    "        \n",
    "    Returns:\n",
    "        df, whose memory usage is reduced.\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns: #columns毎に処理\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def read_data(directory, file_name):\n",
    "    print('Reading files...')\n",
    "    df = pd.read_csv(os.path.sep.join([str(directory), _DATA_DIR, file_name]))\n",
    "    df = reduce_mem_usage(df)\n",
    "    print('{} has {} rows and {} columns'.format(file_name, df.shape[0], df.shape[1]))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id        item_id    dept_id   cat_id  \\\n",
      "0         HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  HOBBIES_1  HOBBIES   \n",
      "1         HOBBIES_1_009_CA_1_validation  HOBBIES_1_009  HOBBIES_1  HOBBIES   \n",
      "2         HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  HOBBIES_1  HOBBIES   \n",
      "3         HOBBIES_1_012_CA_1_validation  HOBBIES_1_012  HOBBIES_1  HOBBIES   \n",
      "4         HOBBIES_1_015_CA_1_validation  HOBBIES_1_015  HOBBIES_1  HOBBIES   \n",
      "...                                 ...            ...        ...      ...   \n",
      "46881672    FOODS_3_823_WI_3_validation    FOODS_3_823    FOODS_3    FOODS   \n",
      "46881673    FOODS_3_824_WI_3_validation    FOODS_3_824    FOODS_3    FOODS   \n",
      "46881674    FOODS_3_825_WI_3_validation    FOODS_3_825    FOODS_3    FOODS   \n",
      "46881675    FOODS_3_826_WI_3_validation    FOODS_3_826    FOODS_3    FOODS   \n",
      "46881676    FOODS_3_827_WI_3_validation    FOODS_3_827    FOODS_3    FOODS   \n",
      "\n",
      "         store_id state_id     d  sales  release  sell_price  price_max  \\\n",
      "0            CA_1       CA     1   12.0        0    0.459961   0.500000   \n",
      "1            CA_1       CA     1    2.0        0    1.559570   1.769531   \n",
      "2            CA_1       CA     1    0.0        0    3.169922   3.169922   \n",
      "3            CA_1       CA     1    0.0        0    5.980469   6.519531   \n",
      "4            CA_1       CA     1    4.0        0    0.700195   0.720215   \n",
      "...           ...      ...   ...    ...      ...         ...        ...   \n",
      "46881672     WI_3       WI  1941    NaN        0    2.980469   2.980469   \n",
      "46881673     WI_3       WI  1941    NaN        0    2.480469   2.679688   \n",
      "46881674     WI_3       WI  1941    NaN        0    3.980469   4.378906   \n",
      "46881675     WI_3       WI  1941    NaN      230    1.280273   1.280273   \n",
      "46881676     WI_3       WI  1941    NaN      304    1.000000   1.000000   \n",
      "\n",
      "          price_min  price_std  price_mean  price_norm  price_nunique  \\\n",
      "0          0.419922   0.019791    0.476318    0.919922            4.0   \n",
      "1          1.559570   0.032715    1.764648    0.881348            2.0   \n",
      "2          2.970703   0.046173    2.982422    1.000000            2.0   \n",
      "3          5.980469   0.115906    6.468750    0.917480            3.0   \n",
      "4          0.680176   0.011353    0.707031    0.972168            3.0   \n",
      "...             ...        ...         ...         ...            ...   \n",
      "46881672   2.480469   0.171875    2.802734    1.000000            5.0   \n",
      "46881673   2.000000   0.252930    2.507812    0.925781            4.0   \n",
      "46881674   3.980469   0.187866    4.117188    0.909180            3.0   \n",
      "46881675   1.280273   0.000000    1.280273    1.000000            1.0   \n",
      "46881676   1.000000   0.000000    1.000000    1.000000            1.0   \n",
      "\n",
      "          item_nunique  price_momentum  price_momentum_m  price_momentum_y  \\\n",
      "0                   16             NaN          0.968750          0.949707   \n",
      "1                    9             NaN          0.885742          0.896484   \n",
      "2                   20             NaN          1.064453          1.043945   \n",
      "3                   71             NaN          0.922363          0.959473   \n",
      "4                   16             NaN          0.990234          1.001953   \n",
      "...                ...             ...               ...               ...   \n",
      "46881672           206             1.0          1.029297          1.022461   \n",
      "46881673           135             1.0          0.997070          1.112305   \n",
      "46881674           150             1.0          0.965820          1.000000   \n",
      "46881675            44             1.0          1.000000          1.000000   \n",
      "46881676           142             1.0          1.000000          1.000000   \n",
      "\n",
      "         event_name_1 event_type_1 event_name_2 event_type_2 snap_CA snap_TX  \\\n",
      "0                 NaN          NaN          NaN          NaN       0       0   \n",
      "1                 NaN          NaN          NaN          NaN       0       0   \n",
      "2                 NaN          NaN          NaN          NaN       0       0   \n",
      "3                 NaN          NaN          NaN          NaN       0       0   \n",
      "4                 NaN          NaN          NaN          NaN       0       0   \n",
      "...               ...          ...          ...          ...     ...     ...   \n",
      "46881672          NaN          NaN          NaN          NaN       0       0   \n",
      "46881673          NaN          NaN          NaN          NaN       0       0   \n",
      "46881674          NaN          NaN          NaN          NaN       0       0   \n",
      "46881675          NaN          NaN          NaN          NaN       0       0   \n",
      "46881676          NaN          NaN          NaN          NaN       0       0   \n",
      "\n",
      "         snap_WI  tm_d  tm_w  tm_m  tm_y  tm_wm  tm_dw  tm_w_end  \n",
      "0              0    29     4     1     0      5      5         1  \n",
      "1              0    29     4     1     0      5      5         1  \n",
      "2              0    29     4     1     0      5      5         1  \n",
      "3              0    29     4     1     0      5      5         1  \n",
      "4              0    29     4     1     0      5      5         1  \n",
      "...          ...   ...   ...   ...   ...    ...    ...       ...  \n",
      "46881672       0    22    20     5     5      4      6         1  \n",
      "46881673       0    22    20     5     5      4      6         1  \n",
      "46881674       0    22    20     5     5      4      6         1  \n",
      "46881675       0    22    20     5     5      4      6         1  \n",
      "46881676       0    22    20     5     5      4      6         1  \n",
      "\n",
      "[46881677 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "parent_dir = pathlib.Path(os.path.abspath(os.curdir)).parent.parent\n",
    "base_df = pd.concat([pd.read_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, BASE])),\n",
    "                pd.read_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, PRICE])).iloc[:,2:],\n",
    "                pd.read_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, CALENDAR])).iloc[:,2:]],\n",
    "                axis=1)\n",
    "\n",
    "print(base_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constant variables for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_NUM_IMPORT_ROWS_FOR_MELT: 22257700\n"
     ]
    }
   ],
   "source": [
    "# The historical data range from 2011-01-29 to 2016-06-19\n",
    "_FIRST_SALES_DATE = date(year=2011, month=1, day=29)\n",
    "\n",
    "# 予測期間とitem数の定義 / number of items, and number of prediction period\n",
    "_NUM_UNIQUE_ITEM_ID = 30490\n",
    "_DAYS_FOR_PREDICTION = 28\n",
    "\n",
    "DAYS_PER_YEAR = 365\n",
    "_NUM_YEARS_FOR_MELT = 2\n",
    "_NUM_IMPORT_ROWS_FOR_MELT = DAYS_PER_YEAR * _NUM_YEARS_FOR_MELT * _NUM_UNIQUE_ITEM_ID\n",
    "print(f\"_NUM_IMPORT_ROWS_FOR_MELT: {_NUM_IMPORT_ROWS_FOR_MELT}\")\n",
    "\n",
    "_SALES_HISTORY_DAYS = 1913 # And we will use last 28 days as validation\n",
    "_SALES_HISTORY_START_DAYS_FOR_VALIDATION = _SALES_HISTORY_DAYS + 1\n",
    "_SALES_HISTORY_START_DAYS_FOR_EVALUATION = 1942\n",
    "\n",
    "TARGET = 'sales' # Our Target\n",
    "MAIN_INDEX = ['id','d']  # We can identify items by these columns\n",
    "USE_PREPROCESSED_DATAFRAME = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent_dir: /home/ec2-user/SageMaker\n",
      "max_num_days: 1941\n",
      "base_df.dtypes: id                  category\n",
      "item_id             category\n",
      "dept_id             category\n",
      "cat_id              category\n",
      "store_id            category\n",
      "state_id            category\n",
      "d                     object\n",
      "sales                float64\n",
      "release                int16\n",
      "sell_price           float16\n",
      "price_max            float16\n",
      "price_min            float16\n",
      "price_std            float16\n",
      "price_mean           float16\n",
      "price_norm           float16\n",
      "price_nunique        float16\n",
      "item_nunique           int16\n",
      "price_momentum       float16\n",
      "price_momentum_m     float16\n",
      "price_momentum_y     float16\n",
      "event_name_1        category\n",
      "event_type_1        category\n",
      "event_name_2        category\n",
      "event_type_2        category\n",
      "snap_CA             category\n",
      "snap_TX             category\n",
      "snap_WI             category\n",
      "tm_d                    int8\n",
      "tm_w                    int8\n",
      "tm_m                    int8\n",
      "tm_y                    int8\n",
      "tm_wm                   int8\n",
      "tm_dw                   int8\n",
      "tm_w_end                int8\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "parent_dir = pathlib.Path(os.path.abspath(os.curdir)).parent.parent\n",
    "print(f\"parent_dir: {parent_dir}\")\n",
    "_EXPORT_FILE_NAME = \"aws_forecast_base_renamed_preprocessed_df.pkl\"\n",
    "\n",
    "# convert \"d\" column to date. the format is like \"2019-01-01\". soruce: https://docs.aws.amazon.com/forecast/latest/dg/forecast.dg.pdf\n",
    "current_date = _FIRST_SALES_DATE\n",
    "max_num_days = base_df['d'].max()\n",
    "print(f\"max_num_days: {max_num_days}\")\n",
    "\n",
    "# change the data format of \"d\" from int to string      \n",
    "base_df['d'] = base_df['d'].astype(str)\n",
    "print(f\"base_df.dtypes: {base_df.dtypes}\")\n",
    "\n",
    "if not USE_PREPROCESSED_DATAFRAME:      \n",
    "    for i in range (1, max_num_days+1):\n",
    "    #     base_df[base_df['d'] == i]['d'] = current_date.strftime(\"%Y-%m-%d\")\n",
    "        base_df.loc[base_df['d'] == str(i), 'd'] = current_date.strftime('%Y-%m-%d')\n",
    "        if i % 10 == 0:\n",
    "            print(f\"i: {i}, datetime.now(): {datetime.now()}\")\n",
    "            print(f\"current_date: {current_date}\")\n",
    "#             print(f\"base_df.loc[base_df['d'] == current_date.strftime('%Y-%m-%d')]: {base_df.loc[base_df['d'] == current_date.strftime('%Y-%m-%d')]}\")\n",
    "\n",
    "        current_date = current_date + timedelta(days=1)\n",
    "\n",
    "    # change the column name\n",
    "    base_renamed_df = base_df.rename(columns={'d': 'timestamp'})\n",
    "    print(f\"base_renamed_df: {base_renamed_df}\") \n",
    "                  \n",
    "    # export the converted dataframe as pickle file\n",
    "    print(\"data export start\")\n",
    "    base_renamed_df.to_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, _EXPORT_FILE_NAME]))\n",
    "    print('data export finished. Size:', base_renamed_df.shape)\n",
    "                  \n",
    "else:\n",
    "    #  read pickle file\n",
    "    pkl_path = os.path.sep.join([str(parent_dir), _DATA_DIR, _EXPORT_FILE_NAME])\n",
    "    base_renamed_df = pd.read_pickle(pkl_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id        item_id    dept_id   cat_id  \\\n",
      "0         HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  HOBBIES_1  HOBBIES   \n",
      "1         HOBBIES_1_009_CA_1_validation  HOBBIES_1_009  HOBBIES_1  HOBBIES   \n",
      "2         HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  HOBBIES_1  HOBBIES   \n",
      "3         HOBBIES_1_012_CA_1_validation  HOBBIES_1_012  HOBBIES_1  HOBBIES   \n",
      "4         HOBBIES_1_015_CA_1_validation  HOBBIES_1_015  HOBBIES_1  HOBBIES   \n",
      "...                                 ...            ...        ...      ...   \n",
      "46881672    FOODS_3_823_WI_3_validation    FOODS_3_823    FOODS_3    FOODS   \n",
      "46881673    FOODS_3_824_WI_3_validation    FOODS_3_824    FOODS_3    FOODS   \n",
      "46881674    FOODS_3_825_WI_3_validation    FOODS_3_825    FOODS_3    FOODS   \n",
      "46881675    FOODS_3_826_WI_3_validation    FOODS_3_826    FOODS_3    FOODS   \n",
      "46881676    FOODS_3_827_WI_3_validation    FOODS_3_827    FOODS_3    FOODS   \n",
      "\n",
      "         store_id state_id   timestamp  sales  sell_price  tm_w_end  \n",
      "0            CA_1       CA  2011-01-29   12.0    0.459961         1  \n",
      "1            CA_1       CA  2011-01-29    2.0    1.559570         1  \n",
      "2            CA_1       CA  2011-01-29    0.0    3.169922         1  \n",
      "3            CA_1       CA  2011-01-29    0.0    5.980469         1  \n",
      "4            CA_1       CA  2011-01-29    4.0    0.700195         1  \n",
      "...           ...      ...         ...    ...         ...       ...  \n",
      "46881672     WI_3       WI  2016-05-22    NaN    2.980469         1  \n",
      "46881673     WI_3       WI  2016-05-22    NaN    2.480469         1  \n",
      "46881674     WI_3       WI  2016-05-22    NaN    3.980469         1  \n",
      "46881675     WI_3       WI  2016-05-22    NaN    1.280273         1  \n",
      "46881676     WI_3       WI  2016-05-22    NaN    1.000000         1  \n",
      "\n",
      "[46881677 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#     id\n",
    "#     item_id\n",
    "#     dept_id\n",
    "#     cat_id\n",
    "#     store_id\n",
    "#     state_id\n",
    "#     d\n",
    "#     sales\n",
    "#     sell_price\n",
    "# tm_dw: day of week <- delete due to the number of feature limitation for aws forecast\n",
    "# tm_w_end: weekend\n",
    "extracted_base_df = base_renamed_df.loc[:,['id','item_id','dept_id','cat_id','store_id','state_id','timestamp','sales','sell_price','tm_w_end']]\n",
    "print(extracted_base_df)\n",
    "\n",
    "extracted_base_without_id_df = extracted_base_df.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reformat the columns for aws forecast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the data type\n",
    "extracted_base_df[\"sell_price\"] = extracted_base_df[\"sell_price\"].astype(str)\n",
    "extracted_base_df[\"tm_w_end\"] = extracted_base_df[\"tm_w_end\"].astype(str)\n",
    "\n",
    "# Target Time Series Dataset Typeには、developers guideに乗っていないFeatureも入れられるがStringしかだめ。\n",
    "# sell_priceはRelated Time Series Dataset Typeとして別のdatasetに入れないとだめ。\n",
    "# change the column name\n",
    "extracted_base_df = extracted_base_df.rename(columns={'item_id': 'item_category'})\n",
    "extracted_base_df = extracted_base_df.rename(columns={'id': 'item_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_renamed_custom_domain_df:                                 item_id  item_category    dept_id   cat_id  \\\n",
      "0         HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  HOBBIES_1  HOBBIES   \n",
      "1         HOBBIES_1_009_CA_1_validation  HOBBIES_1_009  HOBBIES_1  HOBBIES   \n",
      "2         HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  HOBBIES_1  HOBBIES   \n",
      "3         HOBBIES_1_012_CA_1_validation  HOBBIES_1_012  HOBBIES_1  HOBBIES   \n",
      "4         HOBBIES_1_015_CA_1_validation  HOBBIES_1_015  HOBBIES_1  HOBBIES   \n",
      "...                                 ...            ...        ...      ...   \n",
      "46881672    FOODS_3_823_WI_3_validation    FOODS_3_823    FOODS_3    FOODS   \n",
      "46881673    FOODS_3_824_WI_3_validation    FOODS_3_824    FOODS_3    FOODS   \n",
      "46881674    FOODS_3_825_WI_3_validation    FOODS_3_825    FOODS_3    FOODS   \n",
      "46881675    FOODS_3_826_WI_3_validation    FOODS_3_826    FOODS_3    FOODS   \n",
      "46881676    FOODS_3_827_WI_3_validation    FOODS_3_827    FOODS_3    FOODS   \n",
      "\n",
      "         store_id state_id   timestamp  target_value sell_price tm_w_end  \n",
      "0            CA_1       CA  2011-01-29          12.0       0.46        1  \n",
      "1            CA_1       CA  2011-01-29           2.0       1.56        1  \n",
      "2            CA_1       CA  2011-01-29           0.0       3.17        1  \n",
      "3            CA_1       CA  2011-01-29           0.0       5.98        1  \n",
      "4            CA_1       CA  2011-01-29           4.0        0.7        1  \n",
      "...           ...      ...         ...           ...        ...      ...  \n",
      "46881672     WI_3       WI  2016-05-22           NaN       2.98        1  \n",
      "46881673     WI_3       WI  2016-05-22           NaN       2.48        1  \n",
      "46881674     WI_3       WI  2016-05-22           NaN       3.98        1  \n",
      "46881675     WI_3       WI  2016-05-22           NaN       1.28        1  \n",
      "46881676     WI_3       WI  2016-05-22           NaN        1.0        1  \n",
      "\n",
      "[46881677 rows x 10 columns]\n",
      "base_renamed_retail_domain_df:                                 item_id  item_category    dept_id   cat_id  \\\n",
      "0         HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  HOBBIES_1  HOBBIES   \n",
      "1         HOBBIES_1_009_CA_1_validation  HOBBIES_1_009  HOBBIES_1  HOBBIES   \n",
      "2         HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  HOBBIES_1  HOBBIES   \n",
      "3         HOBBIES_1_012_CA_1_validation  HOBBIES_1_012  HOBBIES_1  HOBBIES   \n",
      "4         HOBBIES_1_015_CA_1_validation  HOBBIES_1_015  HOBBIES_1  HOBBIES   \n",
      "...                                 ...            ...        ...      ...   \n",
      "46881672    FOODS_3_823_WI_3_validation    FOODS_3_823    FOODS_3    FOODS   \n",
      "46881673    FOODS_3_824_WI_3_validation    FOODS_3_824    FOODS_3    FOODS   \n",
      "46881674    FOODS_3_825_WI_3_validation    FOODS_3_825    FOODS_3    FOODS   \n",
      "46881675    FOODS_3_826_WI_3_validation    FOODS_3_826    FOODS_3    FOODS   \n",
      "46881676    FOODS_3_827_WI_3_validation    FOODS_3_827    FOODS_3    FOODS   \n",
      "\n",
      "         store_id state_id   timestamp  demand tm_w_end  \n",
      "0            CA_1       CA  2011-01-29    12.0        1  \n",
      "1            CA_1       CA  2011-01-29     2.0        1  \n",
      "2            CA_1       CA  2011-01-29     0.0        1  \n",
      "3            CA_1       CA  2011-01-29     0.0        1  \n",
      "4            CA_1       CA  2011-01-29     4.0        1  \n",
      "...           ...      ...         ...     ...      ...  \n",
      "46881672     WI_3       WI  2016-05-22     NaN        1  \n",
      "46881673     WI_3       WI  2016-05-22     NaN        1  \n",
      "46881674     WI_3       WI  2016-05-22     NaN        1  \n",
      "46881675     WI_3       WI  2016-05-22     NaN        1  \n",
      "46881676     WI_3       WI  2016-05-22     NaN        1  \n",
      "\n",
      "[46881677 rows x 9 columns]\n",
      "base_renamed_retail_domain_related_dataset_df:                                 item_id   timestamp  price\n",
      "0         HOBBIES_1_008_CA_1_validation  2011-01-29   0.46\n",
      "1         HOBBIES_1_009_CA_1_validation  2011-01-29   1.56\n",
      "2         HOBBIES_1_010_CA_1_validation  2011-01-29   3.17\n",
      "3         HOBBIES_1_012_CA_1_validation  2011-01-29   5.98\n",
      "4         HOBBIES_1_015_CA_1_validation  2011-01-29   0.70\n",
      "...                                 ...         ...    ...\n",
      "46881672    FOODS_3_823_WI_3_validation  2016-05-22   2.98\n",
      "46881673    FOODS_3_824_WI_3_validation  2016-05-22   2.48\n",
      "46881674    FOODS_3_825_WI_3_validation  2016-05-22   3.98\n",
      "46881675    FOODS_3_826_WI_3_validation  2016-05-22   1.28\n",
      "46881676    FOODS_3_827_WI_3_validation  2016-05-22   1.00\n",
      "\n",
      "[46881677 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# reformat the custom domain df\n",
    "base_renamed_custom_domain_df = extracted_base_df.rename(columns={'sales': 'target_value'})\n",
    "\n",
    "# reformat the retail domain df\n",
    "base_renamed_retail_domain_df = extracted_base_df.rename(columns={'sales': 'demand'})\n",
    "base_renamed_retail_domain_df = base_renamed_retail_domain_df.rename(columns={'sell_price': 'price'})\n",
    "base_renamed_retail_domain_df[\"price\"] = base_renamed_retail_domain_df[\"price\"].astype('float32')\n",
    "\n",
    "# create retail domain df related dataset\n",
    "base_renamed_retail_domain_related_dataset_df = base_renamed_retail_domain_df[['item_id', 'timestamp', 'price']]\n",
    "base_renamed_retail_domain_df = base_renamed_retail_domain_df.drop('price', axis=1)\n",
    "\n",
    "print(f\"base_renamed_custom_domain_df: {base_renamed_custom_domain_df}\")\n",
    "print(f\"base_renamed_retail_domain_df: {base_renamed_retail_domain_df}\")\n",
    "print(f\"base_renamed_retail_domain_related_dataset_df: {base_renamed_retail_domain_related_dataset_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HOBBIES_1_008_CA_1_validation, HOBBIES_1_009_CA_1_validation, HOBBIES_1_010_CA_1_validation, HOBBIES_1_012_CA_1_validation, HOBBIES_1_015_CA_1_validation, ..., HOUSEHOLD_1_278_CA_3_validation, FOODS_3_595_CA_3_validation, HOUSEHOLD_1_400_CA_4_validation, HOUSEHOLD_1_386_WI_1_validation, HOUSEHOLD_1_020_WI_2_validation]\n",
       "Length: 30490\n",
       "Categories (30490, object): [HOBBIES_1_008_CA_1_validation, HOBBIES_1_009_CA_1_validation, HOBBIES_1_010_CA_1_validation, HOBBIES_1_012_CA_1_validation, ..., FOODS_3_595_CA_3_validation, HOUSEHOLD_1_400_CA_4_validation, HOUSEHOLD_1_386_WI_1_validation, HOUSEHOLD_1_020_WI_2_validation]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "base_renamed_retail_domain_df['item_id'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# upload dataframe to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/m5_accuracy_base_renamed_custom_domain_df.csv\n",
      "file upload started: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/m5_accuracy_base_renamed_custom_domain_df.csv\n",
      "file upload finished\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/m5_accuracy_base_renamed_retail_domain_df.csv\n",
      "file upload started: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/m5_accuracy_base_renamed_retail_domain_df.csv\n",
      "file upload finished\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/m5_accuracy_base_renamed_retail_domain_related_dataset_df.csv\n",
      "file upload started: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/m5_accuracy_base_renamed_retail_domain_related_dataset_df.csv\n",
      "file upload finished\n"
     ]
    }
   ],
   "source": [
    "del base_df, base_renamed_df, extracted_base_df, \n",
    "gc.collect()\n",
    "\n",
    "def write_df_to_s3(df, outpath):\n",
    "    \"\"\"\n",
    "    s3にファイルを書き出す処理\n",
    "    \"\"\"\n",
    "    import s3fs\n",
    "#     key = \"your-aws-access-key\"\n",
    "#     secret = \"your-aws-secret-access-key\"\n",
    "    bytes_to_write = df.to_csv(None, index=False).encode()\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    print(\"file upload started: \" + outpath)\n",
    "    with fs.open(outpath, 'wb') as f:\n",
    "      f.write(bytes_to_write)\n",
    "    print(\"file upload finished\")\n",
    "\n",
    "bucket_name = 'sagemaker-m5-forecasting-okada' # Replace with your s3 bucket name\n",
    "path = 'accuracy/aws_forecast'\n",
    "file_name = \"m5_accuracy_base_renamed_custom_domain_df.csv\"\n",
    "url = 's3://{}/{}/{}'.format(bucket_name, path, file_name)\n",
    "print(f\"url: {url}\")\n",
    "write_df_to_s3(base_renamed_custom_domain_df, url)\n",
    "\n",
    "file_name = \"m5_accuracy_base_renamed_retail_domain_df.csv\"\n",
    "url = 's3://{}/{}/{}'.format(bucket_name, path, file_name)\n",
    "print(f\"url: {url}\")\n",
    "write_df_to_s3(base_renamed_retail_domain_df, url)\n",
    "\n",
    "\n",
    "file_name = \"m5_accuracy_base_renamed_retail_domain_related_dataset_df.csv\"\n",
    "url = 's3://{}/{}/{}'.format(bucket_name, path, file_name)\n",
    "print(f\"url: {url}\")\n",
    "write_df_to_s3(base_renamed_retail_domain_related_dataset_df, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from io import StringIO # python3; python2: BytesIO \n",
    "# import boto3\n",
    "\n",
    "# bucket = 'my_bucket_name' # already created on S3\n",
    "# csv_buffer = StringIO()\n",
    "# df.to_csv(csv_buffer)\n",
    "# s3_resource = boto3.resource('s3')\n",
    "# s3_resource.Object(bucket, 'df.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
