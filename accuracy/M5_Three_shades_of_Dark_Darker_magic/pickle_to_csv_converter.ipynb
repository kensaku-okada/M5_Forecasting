{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import psutil\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function fixing random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    \"\"\"Sets seed to make all processes deterministic     # type: int\n",
    "    \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constant variables for helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_CORES: 16\n"
     ]
    }
   ],
   "source": [
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "print(f\"N_CORES: {N_CORES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  constant variables for data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_DATA_DIR: data/M5_Three_shades_of_Dark_Darker_magic\n"
     ]
    }
   ],
   "source": [
    "# change this var according to the dataset you refer to \n",
    "# path to the source's pickle files\n",
    "# _DATA_DIR = os.path.sep.join([\"data\", \"M5_Three_shades_of_Dark_Darker_magic\", \"sample\"])\n",
    "_DATA_DIR = os.path.sep.join([\"data\", \"M5_Three_shades_of_Dark_Darker_magic\"])\n",
    "_OUTPUT_DIR = os.path.sep.join([\"data\", \"M5_Three_shades_of_Dark_Darker_magic\"])\n",
    "print(f\"_DATA_DIR: {_DATA_DIR}\")\n",
    "_CALENDAR_CSV_FILE = \"calendar.csv\"\n",
    "_SAMPLE_SUBMISSION_CSV_FILE = \"sample_submission.csv\"\n",
    "_SALES_TRAIN_VALIDATION_CSV_FILE = \"sales_train_validation.csv\"\n",
    "_SELL_PRICES_CSV_FILE = \"sell_prices.csv\"\n",
    "\n",
    "#PATHS for Features\n",
    "BASE = \"clearned_base_grid_for_darker_magic.pkl\"\n",
    "PRICE = \"base_grid_with_sales_price_features_for_darker_magic.pkl\"\n",
    "CALENDAR = \"base_grid_with_calendar_features_for_darker_magic.pkl\"\n",
    "\n",
    "LAGS = \"base_grid_with_lag_features_for_28_days.pkl\"\n",
    "MEAN_ENC = \"base_grid_with_mean_encoded_ids_means_stds_for_darker_magic.pkl\"\n",
    "\n",
    "\n",
    "#PATHS for Features made by the reference source\n",
    "# BASE = 'grid_part_1.pkl'\n",
    "# PRICE = 'grid_part_2.pkl'\n",
    "# CALENDAR = 'grid_part_3.pkl'\n",
    "# LAGS = 'lags_df_28.pkl'\n",
    "# MEAN_ENC = 'mean_encoding_df.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model hyperparameters and constant variables for training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "VER = 1                          # Our model version\n",
    "SEED = 42                        # We want all things to be as deterministic as possible\n",
    "seed_everything(SEED)\n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'sales'            # Our target column name\n",
    "START_DAY_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
    "END_DAY_TRAIN   = 1913               # End day of our train set\n",
    "PREDICTION_HORIZON_DAYS = 28                 # Prediction horizon\n",
    "# Use or not pretrained models: make this true after completing model training.\n",
    "# USE_AUX = True  \n",
    "USE_AUX = False\n",
    "\n",
    "# FEATURES to remove.\n",
    "# These features lead to overfit or values not present in test set\n",
    "REMOVE_FEATURES = ['id','state_id','store_id', 'date','wm_yr_wk','d',TARGET]\n",
    "MEAN_STD_FEATURES   = ['enc_cat_id_mean','enc_cat_id_std',\n",
    "                   'enc_dept_id_mean','enc_dept_id_std',\n",
    "                   'enc_item_id_mean','enc_item_id_std'] \n",
    "\n",
    "# AUX(pretrained) Models paths\n",
    "PRETRAINED_MODEL_DIR = 'trained_model'\n",
    "\n",
    "#SPLITS for lags creation\n",
    "SHIFT_DAYS  = 28\n",
    "N_LAGS     = 15\n",
    "LAGS_SPLIT = [col for col in range(SHIFT_DAYS, SHIFT_DAYS + N_LAGS)]\n",
    "ROLLING_SPLIT = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function nicely displaying a head of Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "def display(*dfs, head=True):\n",
    "    for df in dfs:\n",
    "        IPython.display.display(df.head() if head else df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function processing df in multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_df_in_multiprocess(func, t_split):\n",
    "    \"\"\"Process ds in Multiprocess\n",
    "    \n",
    "    \"\"\"\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    print(f\"num_cores: {num_cores}\")\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"メモリ使用量を確認するためのシンプルな「メモリプロファイラ」\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "\n",
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    \"\"\"\n",
    "    dtypesを失わないための連結による結合\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1\n",
    "\n",
    "\n",
    "def get_base_test():\n",
    "    \"\"\"Recombines Test set after training\n",
    "    \n",
    "    \"\"\"\n",
    "    base_test = pd.DataFrame()\n",
    "\n",
    "    for store_id in STORE_IDS:\n",
    "        test_pkl_path = os.path.sep.join([PRETRAINED_MODEL_DIR, 'test_dataset_'+store_id+'.pkl'])\n",
    "        temp_df = pd.read_pickle(test_pkl_path)\n",
    "        temp_df['store_id'] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "    \n",
    "    return base_test\n",
    "\n",
    "\n",
    "\n",
    "##### Helper to make dynamic rolling lags #####\n",
    "def make_lag(lag_day):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'sales_lag_'+str(lag_day)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(lag_day)).astype(np.float16)\n",
    "    return lag_df[[col_name]]\n",
    "\n",
    "\n",
    "def make_lag_roll(lag_day):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    shift_day = lag_day[0]\n",
    "    roll_wind = lag_day[1]\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
    "    return lag_df[[col_name]]\n",
    "##### Helper to make dynamic rolling lags #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    reduce the memory usage of the given dataframe.\n",
    "    https://qiita.com/hiroyuki_kageyama/items/02865616811022f79754\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe\n",
    "        verbose: \n",
    "        \n",
    "    Returns:\n",
    "        df, whose memory usage is reduced.\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns: #columns毎に処理\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def read_csv_data(directory, file_name):\n",
    "    print('Reading files...')\n",
    "    df = pd.read_csv(os.path.sep.join([str(directory), _DATA_DIR, file_name]))\n",
    "    df = reduce_mem_usage(df)\n",
    "    print('{} has {} rows and {} columns'.format(file_name, df.shape[0], df.shape[1]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def read_data_by_store(store):\n",
    "\n",
    "#     # Read and contact basic feature\n",
    "#     df = pd.concat([pd.read_pickle(BASE),\n",
    "#                     pd.read_pickle(PRICE).iloc[:,2:],\n",
    "#                     pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
    "#                     axis=1)\n",
    "\n",
    "    # Read and contact basic feature\n",
    "    parent_dir = pathlib.Path(os.path.abspath(os.curdir)).parent.parent\n",
    "    df = pd.concat([pd.read_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, BASE])),\n",
    "                    pd.read_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, PRICE])).iloc[:,2:],\n",
    "                    pd.read_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, CALENDAR])).iloc[:,2:]],\n",
    "                    axis=1)\n",
    "    \n",
    "    # Leave only relevant store\n",
    "    df = df[df['store_id']==store]\n",
    "\n",
    "    # With memory limits we have to read lags and mean encoding features separately and drop items that we don't need.\n",
    "    # As our Features Grids are aligned \n",
    "    # we can use index to keep only necessary rows\n",
    "    # Alignment is good for us as concat uses less memory than merge.\n",
    "    df2 = pd.read_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, MEAN_ENC]))[MEAN_STD_FEATURES]\n",
    "    df2 = df2[df2.index.isin(df.index)]\n",
    "    \n",
    "    df3 = pd.read_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, LAGS])).iloc[:,3:]\n",
    "    df3 = df3[df3.index.isin(df.index)]\n",
    "    \n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "    del df2 # to not reach memory limit \n",
    "    \n",
    "    df = pd.concat([df, df3], axis=1)\n",
    "    del df3 # to not reach memory limit \n",
    "    \n",
    "    # Create features list\n",
    "    features = [col for col in list(df) if col not in REMOVE_FEATURES]\n",
    "    df = df[['id','d',TARGET]+features]\n",
    "    \n",
    "    # Skipping first n rows\n",
    "    df = df[df['d']>=START_DAY_TRAIN].reset_index(drop=True)\n",
    "    \n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_columns: ['item_id', 'dept_id', 'cat_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60']\n",
      "features_columns: ['item_id', 'dept_id', 'cat_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60']\n",
      "features_columns: ['item_id', 'dept_id', 'cat_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60']\n",
      "features_columns: ['item_id', 'dept_id', 'cat_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60']\n",
      "features_columns: ['item_id', 'dept_id', 'cat_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60']\n",
      "features_columns: ['item_id', 'dept_id', 'cat_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60']\n",
      "features_columns: ['item_id', 'dept_id', 'cat_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60']\n",
      "features_columns: ['item_id', 'dept_id', 'cat_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60']\n",
      "features_columns: ['item_id', 'dept_id', 'cat_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60']\n",
      "features_columns: ['item_id', 'dept_id', 'cat_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60']\n"
     ]
    }
   ],
   "source": [
    "STORE_IDS = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n",
    "\n",
    "all_stores_df = pd.DataFrame()\n",
    "all_stores_wo_store_id_df = pd.DataFrame()\n",
    "\n",
    "for store_id in STORE_IDS:\n",
    "    # Get grid for current store\n",
    "    grid_df, features_columns = read_data_by_store(store_id)\n",
    "# todo: when using aws forecast per store_id SEPARATELY, add store_id to the dataframe\n",
    "#     grid_df[\"store_id\"] = store_id\n",
    "\n",
    "    print(f\"features_columns: {features_columns}\")\n",
    "    \n",
    "    all_stores_df = pd.concat([all_stores_df, grid_df], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_stores_df.dtypes: id                        category\n",
      "d                            int16\n",
      "sales                      float64\n",
      "item_id                   category\n",
      "dept_id                   category\n",
      "cat_id                    category\n",
      "release                      int16\n",
      "sell_price                 float16\n",
      "price_max                  float16\n",
      "price_min                  float16\n",
      "price_std                  float16\n",
      "price_mean                 float16\n",
      "price_norm                 float16\n",
      "price_nunique              float16\n",
      "item_nunique                 int16\n",
      "price_momentum             float16\n",
      "price_momentum_m           float16\n",
      "price_momentum_y           float16\n",
      "event_name_1              category\n",
      "event_type_1              category\n",
      "event_name_2              category\n",
      "event_type_2              category\n",
      "snap_CA                   category\n",
      "snap_TX                   category\n",
      "snap_WI                   category\n",
      "tm_d                          int8\n",
      "tm_w                          int8\n",
      "tm_m                          int8\n",
      "tm_y                          int8\n",
      "tm_wm                         int8\n",
      "tm_dw                         int8\n",
      "tm_w_end                      int8\n",
      "enc_cat_id_mean            float16\n",
      "enc_cat_id_std             float16\n",
      "enc_dept_id_mean           float16\n",
      "enc_dept_id_std            float16\n",
      "enc_item_id_mean           float16\n",
      "enc_item_id_std            float16\n",
      "sales_lag_28               float16\n",
      "sales_lag_29               float16\n",
      "sales_lag_30               float16\n",
      "sales_lag_31               float16\n",
      "sales_lag_32               float16\n",
      "sales_lag_33               float16\n",
      "sales_lag_34               float16\n",
      "sales_lag_35               float16\n",
      "sales_lag_36               float16\n",
      "sales_lag_37               float16\n",
      "sales_lag_38               float16\n",
      "sales_lag_39               float16\n",
      "sales_lag_40               float16\n",
      "sales_lag_41               float16\n",
      "sales_lag_42               float16\n",
      "rolling_mean_7             float16\n",
      "rolling_std_7              float16\n",
      "rolling_mean_14            float16\n",
      "rolling_std_14             float16\n",
      "rolling_mean_30            float16\n",
      "rolling_std_30             float16\n",
      "rolling_mean_60            float16\n",
      "rolling_std_60             float16\n",
      "rolling_mean_180           float16\n",
      "rolling_std_180            float16\n",
      "rolling_mean_tmp_1_7       float16\n",
      "rolling_mean_tmp_1_14      float16\n",
      "rolling_mean_tmp_1_30      float16\n",
      "rolling_mean_tmp_1_60      float16\n",
      "rolling_mean_tmp_7_7       float16\n",
      "rolling_mean_tmp_7_14      float16\n",
      "rolling_mean_tmp_7_30      float16\n",
      "rolling_mean_tmp_7_60      float16\n",
      "rolling_mean_tmp_14_7      float16\n",
      "rolling_mean_tmp_14_14     float16\n",
      "rolling_mean_tmp_14_30     float16\n",
      "rolling_mean_tmp_14_60     float16\n",
      "dtype: object\n",
      "all_stores_df:                                     id     d  sales        item_id    dept_id  \\\n",
      "0        HOBBIES_1_008_CA_1_validation     1   12.0  HOBBIES_1_008  HOBBIES_1   \n",
      "1        HOBBIES_1_009_CA_1_validation     1    2.0  HOBBIES_1_009  HOBBIES_1   \n",
      "2        HOBBIES_1_010_CA_1_validation     1    0.0  HOBBIES_1_010  HOBBIES_1   \n",
      "3        HOBBIES_1_012_CA_1_validation     1    0.0  HOBBIES_1_012  HOBBIES_1   \n",
      "4        HOBBIES_1_015_CA_1_validation     1    4.0  HOBBIES_1_015  HOBBIES_1   \n",
      "...                                ...   ...    ...            ...        ...   \n",
      "4772036    FOODS_3_823_WI_3_validation  1941    NaN    FOODS_3_823    FOODS_3   \n",
      "4772037    FOODS_3_824_WI_3_validation  1941    NaN    FOODS_3_824    FOODS_3   \n",
      "4772038    FOODS_3_825_WI_3_validation  1941    NaN    FOODS_3_825    FOODS_3   \n",
      "4772039    FOODS_3_826_WI_3_validation  1941    NaN    FOODS_3_826    FOODS_3   \n",
      "4772040    FOODS_3_827_WI_3_validation  1941    NaN    FOODS_3_827    FOODS_3   \n",
      "\n",
      "          cat_id  release  sell_price  price_max  price_min  price_std  \\\n",
      "0        HOBBIES        0    0.459961   0.500000   0.419922   0.019791   \n",
      "1        HOBBIES        0    1.559570   1.769531   1.559570   0.032715   \n",
      "2        HOBBIES        0    3.169922   3.169922   2.970703   0.046173   \n",
      "3        HOBBIES        0    5.980469   6.519531   5.980469   0.115906   \n",
      "4        HOBBIES        0    0.700195   0.720215   0.680176   0.011353   \n",
      "...          ...      ...         ...        ...        ...        ...   \n",
      "4772036    FOODS        0    2.980469   2.980469   2.480469   0.171875   \n",
      "4772037    FOODS        0    2.480469   2.679688   2.000000   0.252930   \n",
      "4772038    FOODS        0    3.980469   4.378906   3.980469   0.187866   \n",
      "4772039    FOODS      230    1.280273   1.280273   1.280273   0.000000   \n",
      "4772040    FOODS      304    1.000000   1.000000   1.000000   0.000000   \n",
      "\n",
      "         price_mean  price_norm  price_nunique  item_nunique  price_momentum  \\\n",
      "0          0.476318    0.919922            4.0            16             NaN   \n",
      "1          1.764648    0.881348            2.0             9             NaN   \n",
      "2          2.982422    1.000000            2.0            20             NaN   \n",
      "3          6.468750    0.917480            3.0            71             NaN   \n",
      "4          0.707031    0.972168            3.0            16             NaN   \n",
      "...             ...         ...            ...           ...             ...   \n",
      "4772036    2.802734    1.000000            5.0           206             1.0   \n",
      "4772037    2.507812    0.925781            4.0           135             1.0   \n",
      "4772038    4.117188    0.909180            3.0           150             1.0   \n",
      "4772039    1.280273    1.000000            1.0            44             1.0   \n",
      "4772040    1.000000    1.000000            1.0           142             1.0   \n",
      "\n",
      "         price_momentum_m  price_momentum_y event_name_1 event_type_1  \\\n",
      "0                0.968750          0.949707          NaN          NaN   \n",
      "1                0.885742          0.896484          NaN          NaN   \n",
      "2                1.064453          1.043945          NaN          NaN   \n",
      "3                0.922363          0.959473          NaN          NaN   \n",
      "4                0.990234          1.001953          NaN          NaN   \n",
      "...                   ...               ...          ...          ...   \n",
      "4772036          1.029297          1.022461          NaN          NaN   \n",
      "4772037          0.997070          1.112305          NaN          NaN   \n",
      "4772038          0.965820          1.000000          NaN          NaN   \n",
      "4772039          1.000000          1.000000          NaN          NaN   \n",
      "4772040          1.000000          1.000000          NaN          NaN   \n",
      "\n",
      "        event_name_2 event_type_2 snap_CA snap_TX snap_WI  tm_d  tm_w  tm_m  \\\n",
      "0                NaN          NaN       0       0       0    29     4     1   \n",
      "1                NaN          NaN       0       0       0    29     4     1   \n",
      "2                NaN          NaN       0       0       0    29     4     1   \n",
      "3                NaN          NaN       0       0       0    29     4     1   \n",
      "4                NaN          NaN       0       0       0    29     4     1   \n",
      "...              ...          ...     ...     ...     ...   ...   ...   ...   \n",
      "4772036          NaN          NaN       0       0       0    22    20     5   \n",
      "4772037          NaN          NaN       0       0       0    22    20     5   \n",
      "4772038          NaN          NaN       0       0       0    22    20     5   \n",
      "4772039          NaN          NaN       0       0       0    22    20     5   \n",
      "4772040          NaN          NaN       0       0       0    22    20     5   \n",
      "\n",
      "         tm_y  tm_wm  tm_dw  tm_w_end  enc_cat_id_mean  enc_cat_id_std  \\\n",
      "0           0      5      5         1         0.708984        2.259766   \n",
      "1           0      5      5         1         0.708984        2.259766   \n",
      "2           0      5      5         1         0.708984        2.259766   \n",
      "3           0      5      5         1         0.708984        2.259766   \n",
      "4           0      5      5         1         0.708984        2.259766   \n",
      "...       ...    ...    ...       ...              ...             ...   \n",
      "4772036     5      4      6         1         2.109375        5.769531   \n",
      "4772037     5      4      6         1         2.109375        5.769531   \n",
      "4772038     5      4      6         1         2.109375        5.769531   \n",
      "4772039     5      4      6         1         2.109375        5.769531   \n",
      "4772040     5      4      6         1         2.109375        5.769531   \n",
      "\n",
      "         enc_dept_id_mean  enc_dept_id_std  enc_item_id_mean  enc_item_id_std  \\\n",
      "0                0.865234         2.544922          4.695312         7.183594   \n",
      "1                0.865234         2.544922          0.850098         1.754883   \n",
      "2                0.865234         2.544922          0.611328         0.863281   \n",
      "3                0.865234         2.544922          0.384766         0.692871   \n",
      "4                0.865234         2.544922          4.441406         6.703125   \n",
      "...                   ...              ...               ...              ...   \n",
      "4772036          2.626953         7.058594          0.846680         1.752930   \n",
      "4772037          2.626953         7.058594          0.434326         0.947266   \n",
      "4772038          2.626953         7.058594          0.700684         1.198242   \n",
      "4772039          2.626953         7.058594          1.108398         1.479492   \n",
      "4772040          2.626953         7.058594          1.644531         3.115234   \n",
      "\n",
      "         sales_lag_28  sales_lag_29  sales_lag_30  sales_lag_31  sales_lag_32  \\\n",
      "0                 NaN           NaN           NaN           NaN           NaN   \n",
      "1                 NaN           NaN           NaN           NaN           NaN   \n",
      "2                 NaN           NaN           NaN           NaN           NaN   \n",
      "3                 NaN           NaN           NaN           NaN           NaN   \n",
      "4                 NaN           NaN           NaN           NaN           NaN   \n",
      "...               ...           ...           ...           ...           ...   \n",
      "4772036           1.0           0.0           0.0           1.0           0.0   \n",
      "4772037           0.0           1.0           0.0           0.0           0.0   \n",
      "4772038           0.0           1.0           0.0           0.0           1.0   \n",
      "4772039           3.0           1.0           3.0           0.0           1.0   \n",
      "4772040           0.0           0.0           0.0           0.0           0.0   \n",
      "\n",
      "         sales_lag_33  sales_lag_34  sales_lag_35  sales_lag_36  sales_lag_37  \\\n",
      "0                 NaN           NaN           NaN           NaN           NaN   \n",
      "1                 NaN           NaN           NaN           NaN           NaN   \n",
      "2                 NaN           NaN           NaN           NaN           NaN   \n",
      "3                 NaN           NaN           NaN           NaN           NaN   \n",
      "4                 NaN           NaN           NaN           NaN           NaN   \n",
      "...               ...           ...           ...           ...           ...   \n",
      "4772036           0.0           0.0           0.0           0.0           2.0   \n",
      "4772037           0.0           0.0           0.0           0.0           0.0   \n",
      "4772038           0.0           2.0           0.0           1.0           2.0   \n",
      "4772039           0.0           0.0           1.0           0.0           0.0   \n",
      "4772040           0.0           0.0           0.0           0.0           0.0   \n",
      "\n",
      "         sales_lag_38  sales_lag_39  sales_lag_40  sales_lag_41  sales_lag_42  \\\n",
      "0                 NaN           NaN           NaN           NaN           NaN   \n",
      "1                 NaN           NaN           NaN           NaN           NaN   \n",
      "2                 NaN           NaN           NaN           NaN           NaN   \n",
      "3                 NaN           NaN           NaN           NaN           NaN   \n",
      "4                 NaN           NaN           NaN           NaN           NaN   \n",
      "...               ...           ...           ...           ...           ...   \n",
      "4772036           0.0           0.0           0.0           2.0           0.0   \n",
      "4772037           0.0           0.0           1.0           1.0           1.0   \n",
      "4772038           0.0           0.0           0.0           4.0           2.0   \n",
      "4772039           3.0           0.0           1.0           0.0           2.0   \n",
      "4772040           0.0           0.0           0.0           0.0           0.0   \n",
      "\n",
      "         rolling_mean_7  rolling_std_7  rolling_mean_14  rolling_std_14  \\\n",
      "0                   NaN            NaN              NaN             NaN   \n",
      "1                   NaN            NaN              NaN             NaN   \n",
      "2                   NaN            NaN              NaN             NaN   \n",
      "3                   NaN            NaN              NaN             NaN   \n",
      "4                   NaN            NaN              NaN             NaN   \n",
      "...                 ...            ...              ...             ...   \n",
      "4772036        0.285645       0.488037         0.428467    7.558594e-01   \n",
      "4772037        0.142822       0.377930         0.214233    4.257812e-01   \n",
      "4772038        0.571289       0.786621         0.785645    1.188477e+00   \n",
      "4772039        1.142578       1.344727         0.928711    1.207031e+00   \n",
      "4772040        0.000000       0.000000         0.000000    1.192093e-07   \n",
      "\n",
      "         rolling_mean_30  rolling_std_30  rolling_mean_60  rolling_std_60  \\\n",
      "0                    NaN             NaN              NaN             NaN   \n",
      "1                    NaN             NaN              NaN             NaN   \n",
      "2                    NaN             NaN              NaN             NaN   \n",
      "3                    NaN             NaN              NaN             NaN   \n",
      "4                    NaN             NaN              NaN             NaN   \n",
      "...                  ...             ...              ...             ...   \n",
      "4772036         0.199951        0.550781         0.250000        0.571289   \n",
      "4772037         0.300049        0.535156         0.150024        0.404541   \n",
      "4772038         0.866699        1.136719         1.033203        1.056641   \n",
      "4772039         1.066406        1.172852         1.016602        1.065430   \n",
      "4772040         1.166992        1.821289         1.500000        1.770508   \n",
      "\n",
      "         rolling_mean_180  rolling_std_180  rolling_mean_tmp_1_7  \\\n",
      "0                     NaN              NaN                   NaN   \n",
      "1                     NaN              NaN                   NaN   \n",
      "2                     NaN              NaN                   NaN   \n",
      "3                     NaN              NaN                   NaN   \n",
      "4                     NaN              NaN                   NaN   \n",
      "...                   ...              ...                   ...   \n",
      "4772036          0.616699         0.987305                   NaN   \n",
      "4772037          0.049988         0.242798                   NaN   \n",
      "4772038          0.761230         0.941895                   NaN   \n",
      "4772039          1.338867         1.550781                   NaN   \n",
      "4772040          1.510742         1.741211                   NaN   \n",
      "\n",
      "         rolling_mean_tmp_1_14  rolling_mean_tmp_1_30  rolling_mean_tmp_1_60  \\\n",
      "0                          NaN                    NaN                    NaN   \n",
      "1                          NaN                    NaN                    NaN   \n",
      "2                          NaN                    NaN                    NaN   \n",
      "3                          NaN                    NaN                    NaN   \n",
      "4                          NaN                    NaN                    NaN   \n",
      "...                        ...                    ...                    ...   \n",
      "4772036                    NaN                    NaN                    NaN   \n",
      "4772037                    NaN                    NaN                    NaN   \n",
      "4772038                    NaN                    NaN                    NaN   \n",
      "4772039                    NaN                    NaN                    NaN   \n",
      "4772040                    NaN                    NaN                    NaN   \n",
      "\n",
      "         rolling_mean_tmp_7_7  rolling_mean_tmp_7_14  rolling_mean_tmp_7_30  \\\n",
      "0                         NaN                    NaN                    NaN   \n",
      "1                         NaN                    NaN                    NaN   \n",
      "2                         NaN                    NaN                    NaN   \n",
      "3                         NaN                    NaN                    NaN   \n",
      "4                         NaN                    NaN                    NaN   \n",
      "...                       ...                    ...                    ...   \n",
      "4772036                   NaN                    NaN                    NaN   \n",
      "4772037                   NaN                    NaN                    NaN   \n",
      "4772038                   NaN                    NaN                    NaN   \n",
      "4772039                   NaN                    NaN                    NaN   \n",
      "4772040                   NaN                    NaN                    NaN   \n",
      "\n",
      "         rolling_mean_tmp_7_60  rolling_mean_tmp_14_7  rolling_mean_tmp_14_14  \\\n",
      "0                          NaN                    NaN                     NaN   \n",
      "1                          NaN                    NaN                     NaN   \n",
      "2                          NaN                    NaN                     NaN   \n",
      "3                          NaN                    NaN                     NaN   \n",
      "4                          NaN                    NaN                     NaN   \n",
      "...                        ...                    ...                     ...   \n",
      "4772036                    NaN                    NaN                     NaN   \n",
      "4772037                    NaN                    NaN                     NaN   \n",
      "4772038                    NaN                    NaN                     NaN   \n",
      "4772039                    NaN                    NaN                     NaN   \n",
      "4772040                    NaN                    NaN                     NaN   \n",
      "\n",
      "         rolling_mean_tmp_14_30  rolling_mean_tmp_14_60  \n",
      "0                           NaN                     NaN  \n",
      "1                           NaN                     NaN  \n",
      "2                           NaN                     NaN  \n",
      "3                           NaN                     NaN  \n",
      "4                           NaN                     NaN  \n",
      "...                         ...                     ...  \n",
      "4772036                     NaN                     NaN  \n",
      "4772037                     NaN                     NaN  \n",
      "4772038                     NaN                     NaN  \n",
      "4772039                     NaN                     NaN  \n",
      "4772040                     NaN                     NaN  \n",
      "\n",
      "[46881677 rows x 75 columns]\n"
     ]
    }
   ],
   "source": [
    "print(f\"all_stores_df.dtypes: {all_stores_df.dtypes}\")\n",
    "print(f\"all_stores_df: {all_stores_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: add timestamp for aws forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export result to the local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv data export start: 2020-06-27 03:37:21.299022\n",
      "csv data export finished. Size: (46881677, 75)\n",
      "csv data export end: 2020-06-27 04:05:36.341850\n"
     ]
    }
   ],
   "source": [
    "parent_dir = pathlib.Path(os.path.abspath(os.curdir)).parent.parent\n",
    "\n",
    "# # Reading competition sample submission and merging our predictions\n",
    "# # As we have predictions only for \"_validation\" data we need to do fillna() for \"_evaluation\" items\n",
    "# submission_df = read_csv_data(parent_dir, _SAMPLE_SUBMISSION_CSV_FILE)\n",
    "# submission_ids_df = submission_df[[\"id\"]]\n",
    "# display(submission_ids_df)\n",
    "# my_submission_df = submission_ids_df.merge(all_preds, on=['id'], how='left').fillna(0)\n",
    "\n",
    "_EXPORT_FILE_NAME = 'aws_forecast_data_v'+str(VER)+'.csv'\n",
    "print(f\"csv data export start: {datetime.now()}\")\n",
    "all_stores_df.to_csv(os.path.sep.join([str(parent_dir), _OUTPUT_DIR, _EXPORT_FILE_NAME]), index=False)\n",
    "print('csv data export finished. Size:', all_stores_df.shape)\n",
    "print(f\"csv data export end: {datetime.now()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export result to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_path: /home/ec2-user/SageMaker/data/M5_Three_shades_of_Dark_Darker_magic/aws_forecast_data_v1.csv\n",
      "data_location: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/\n",
      "file export start\n",
      "file export finished\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "# import boto3\n",
    "\n",
    "parent_dir = pathlib.Path(os.path.abspath(os.curdir)).parent.parent\n",
    "local_path = os.path.sep.join([str(parent_dir), _DATA_DIR, _EXPORT_FILE_NAME])\n",
    "print(f\"local_path: {local_path}\")\n",
    "\n",
    "# role = sagemaker.get_execution_role()\n",
    "bucket='sagemaker-m5-forecasting-okada'\n",
    "data_directory = 'accuracy/aws_forecast/'\n",
    "data_path = data_directory\n",
    "data_location = 's3://{}/{}'.format(bucket, data_path)\n",
    "print(f\"data_location: {data_location}\")\n",
    "\n",
    "# https://stackoverflow.com/questions/56799763/uploading-a-dataframe-to-aws-s3-bucket-from-sagemaker\n",
    "print(\"file export start\")\n",
    "sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path,\n",
    "    desired_s3_uri=data_location\n",
    ")\n",
    "sagemaker.s3.S3Downloader.list(data_location)\n",
    "print(\"file export finished\")\n",
    "\n",
    "# csv_s3_uri = sagemaker.s3.S3Uploader.upload_string_as_file_body(\n",
    "#     body=all_stores_df.to_csv(index=False),\n",
    "#     desired_s3_uri=data_location\n",
    "# )\n",
    "# print(\"file export start\")\n",
    "# sagemaker.s3.S3Downloader.list(csv_s3_uri)\n",
    "# print(\"file export finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
