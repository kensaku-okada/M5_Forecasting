{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function nicely diplaying a head of Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "def display(*dfs, head=True):\n",
    "    for df in dfs:\n",
    "        IPython.display.display(df.head() if head else df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'sagemaker-m5-forecasting-okada'\n",
    "path = 'accuracy/aws_forecast/result'\n",
    "prefix = \"m5_accuracy_base_renamed_custom_domain_df_result_2020-08-01T06-33-26Z_part\"\n",
    "_EXPORT_FILE_NAME = 'submission_aws_forecast_custom_domain_validation.csv'\n",
    "\n",
    "# prefix = \"m5_accuracy_base_renamed_custom_domain_df_result_2020-08-01T06-33-26Z_part\"\n",
    "# _EXPORT_FILE_NAME = 'submission_aws_forecast_custom_domain_validation.csv'\n",
    "prefix = \"m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part\"\n",
    "_EXPORT_FILE_NAME = 'submission_aws_forecast_retail_domain_validation.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    reduce the memory usage of the given dataframe.\n",
    "    https://qiita.com/hiroyuki_kageyama/items/02865616811022f79754\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe\n",
    "        verbose: \n",
    "        \n",
    "    Returns:\n",
    "        df, whose memory usage is reduced.\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns: #columns毎に処理\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def read_csv_data(directory, file_name):\n",
    "    print('Reading files...')\n",
    "    df = pd.read_csv(os.path.sep.join([str(directory), _DATA_DIR, file_name]))\n",
    "    df = reduce_mem_usage(df)\n",
    "    print('{} has {} rows and {} columns'.format(file_name, df.shape[0], df.shape[1]))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    \"\"\"\n",
    "    dtypesを失わないための連結による結合\n",
    "    \n",
    "    \"\"\"\n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import forecast result from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part0.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part1.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part2.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part3.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part4.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part5.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part6.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part7.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part8.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part9.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part10.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part11.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part12.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part13.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part14.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part15.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part16.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part17.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part18.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part19.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part20.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part21.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part22.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part23.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part24.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part25.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part26.csv\n",
      "url: s3://sagemaker-m5-forecasting-okada/accuracy/aws_forecast/result/m5_accuracy_base_renamed_retail_domain_df_2020-08-02T06-45-17Z_part27.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>p50</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>foods_3_721_tx_2_validation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-05-23T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>foods_3_721_tx_2_validation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-05-24T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foods_3_721_tx_2_validation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-05-25T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>foods_3_721_tx_2_validation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-05-26T00:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>foods_3_721_tx_2_validation</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-05-27T00:00:00Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id  p50                  date\n",
       "0  foods_3_721_tx_2_validation  0.0  2016-05-23T00:00:00Z\n",
       "1  foods_3_721_tx_2_validation  0.0  2016-05-24T00:00:00Z\n",
       "2  foods_3_721_tx_2_validation  0.0  2016-05-25T00:00:00Z\n",
       "3  foods_3_721_tx_2_validation  0.0  2016-05-26T00:00:00Z\n",
       "4  foods_3_721_tx_2_validation  0.0  2016-05-27T00:00:00Z"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "853720\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/37703634/how-to-import-a-text-file-on-aws-s3-into-pandas-without-writing-to-disk\n",
    "\n",
    "num_result_files = 28\n",
    "result_df = pd.DataFrame()\n",
    "for i in range (0, num_result_files): \n",
    "    file_name = prefix + str(i) + \".csv\"\n",
    "    url = 's3://{}/{}/{}'.format(bucket_name, path, file_name)\n",
    "    print(f\"url: {url}\")\n",
    "    df = pd.read_csv(url)\n",
    "    df = df[[\"item_id\", \"p50\", \"date\"]]\n",
    "    result_df = pd.concat([result_df, df], axis=0)\n",
    "    \n",
    "result_df = result_df.rename(columns={'item_id': 'id'})\n",
    "    \n",
    "display(result_df)\n",
    "print(len(result_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             id  p50                  date\n",
      "0   foods_3_721_tx_2_validation  0.0  2016-05-23T00:00:00Z\n",
      "1   foods_3_721_tx_2_validation  0.0  2016-05-24T00:00:00Z\n",
      "2   foods_3_721_tx_2_validation  0.0  2016-05-25T00:00:00Z\n",
      "3   foods_3_721_tx_2_validation  0.0  2016-05-26T00:00:00Z\n",
      "4   foods_3_721_tx_2_validation  0.0  2016-05-27T00:00:00Z\n",
      "5   foods_3_721_tx_2_validation  0.0  2016-05-28T00:00:00Z\n",
      "6   foods_3_721_tx_2_validation  0.0  2016-05-29T00:00:00Z\n",
      "7   foods_3_721_tx_2_validation  0.0  2016-05-30T00:00:00Z\n",
      "8   foods_3_721_tx_2_validation  0.0  2016-05-31T00:00:00Z\n",
      "9   foods_3_721_tx_2_validation  0.0  2016-06-01T00:00:00Z\n",
      "10  foods_3_721_tx_2_validation  0.0  2016-06-02T00:00:00Z\n",
      "11  foods_3_721_tx_2_validation  0.0  2016-06-03T00:00:00Z\n",
      "12  foods_3_721_tx_2_validation  0.0  2016-06-04T00:00:00Z\n",
      "13  foods_3_721_tx_2_validation  0.0  2016-06-05T00:00:00Z\n",
      "14  foods_3_721_tx_2_validation  0.0  2016-06-06T00:00:00Z\n",
      "15  foods_3_721_tx_2_validation  0.0  2016-06-07T00:00:00Z\n",
      "16  foods_3_721_tx_2_validation  0.0  2016-06-08T00:00:00Z\n",
      "17  foods_3_721_tx_2_validation  0.0  2016-06-09T00:00:00Z\n",
      "18  foods_3_721_tx_2_validation  0.0  2016-06-10T00:00:00Z\n",
      "19  foods_3_721_tx_2_validation  0.0  2016-06-11T00:00:00Z\n",
      "20  foods_3_721_tx_2_validation  0.0  2016-06-12T00:00:00Z\n",
      "21  foods_3_721_tx_2_validation  0.0  2016-06-13T00:00:00Z\n",
      "22  foods_3_721_tx_2_validation  0.0  2016-06-14T00:00:00Z\n",
      "23  foods_3_721_tx_2_validation  0.0  2016-06-15T00:00:00Z\n",
      "24  foods_3_721_tx_2_validation  0.0  2016-06-16T00:00:00Z\n",
      "25  foods_3_721_tx_2_validation  0.0  2016-06-17T00:00:00Z\n",
      "26  foods_3_721_tx_2_validation  0.0  2016-06-18T00:00:00Z\n",
      "27  foods_3_721_tx_2_validation  0.0  2016-06-19T00:00:00Z\n"
     ]
    }
   ],
   "source": [
    "print(result_df[result_df[\"id\"] == \"foods_3_721_tx_2_validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foods_3_721_tx_2_validation' 'household_2_225_ca_1_validation'\n",
      " 'foods_1_169_ca_4_validation' ... 'hobbies_2_079_ca_3_validation'\n",
      " 'hobbies_2_140_wi_3_validation' 'foods_3_537_wi_2_validation']\n",
      "30490\n",
      "['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']\n",
      "                                    id   F1   F2   F3   F4   F5   F6   F7  \\\n",
      "0          foods_3_721_tx_2_validation  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1      household_2_225_ca_1_validation  2.0  2.0  1.0  1.0  2.0  2.0  2.0   \n",
      "2          foods_1_169_ca_4_validation  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "3        hobbies_2_066_wi_2_validation  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "4      household_1_137_wi_2_validation  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "...                                ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "30485      foods_3_756_tx_3_validation  7.0  6.0  6.0  7.0  7.0  9.0  8.0   \n",
      "30486  household_2_069_wi_1_validation  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "30487    hobbies_2_079_ca_3_validation  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "30488    hobbies_2_140_wi_3_validation  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "30489      foods_3_537_wi_2_validation  1.0  1.0  1.0  0.0  1.0  1.0  1.0   \n",
      "\n",
      "        F8   F9  ...  F19  F20  F21  F22  F23  F24  F25  F26  F27  F28  \n",
      "0      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1      2.0  1.0  ...  2.0  2.0  2.0  1.0  2.0  1.0  1.0  1.0  2.0  2.0  \n",
      "2      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "30485  6.0  7.0  ...  7.0  9.0  8.0  7.0  6.0  5.0  7.0  7.0  8.0  8.0  \n",
      "30486  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "30487  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "30488  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "30489  0.0  1.0  ...  1.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  \n",
      "\n",
      "[30490 rows x 29 columns]\n",
      "output_df.shape: (30490, 29)\n"
     ]
    }
   ],
   "source": [
    "id_list = result_df['id'].unique()\n",
    "print(id_list)\n",
    "print(len(id_list))\n",
    "\n",
    "num_prediction_days = 28\n",
    "column_list = [\"F\" + str(i) for i in range (1, num_prediction_days + 1)]\n",
    "print(column_list)\n",
    "\n",
    "date_list = [\n",
    "\"2016-05-23T00:00:00Z\",\n",
    "\"2016-05-24T00:00:00Z\",\n",
    "\"2016-05-25T00:00:00Z\",\n",
    "\"2016-05-26T00:00:00Z\",\n",
    "\"2016-05-27T00:00:00Z\",\n",
    "\"2016-05-28T00:00:00Z\",\n",
    "\"2016-05-29T00:00:00Z\",\n",
    "\"2016-05-30T00:00:00Z\",\n",
    "\"2016-05-31T00:00:00Z\",\n",
    "\"2016-06-01T00:00:00Z\",\n",
    "\"2016-06-02T00:00:00Z\",\n",
    "\"2016-06-03T00:00:00Z\",\n",
    "\"2016-06-04T00:00:00Z\",\n",
    "\"2016-06-05T00:00:00Z\",\n",
    "\"2016-06-06T00:00:00Z\",\n",
    "\"2016-06-07T00:00:00Z\",\n",
    "\"2016-06-08T00:00:00Z\",\n",
    "\"2016-06-09T00:00:00Z\",\n",
    "\"2016-06-10T00:00:00Z\",\n",
    "\"2016-06-11T00:00:00Z\",\n",
    "\"2016-06-12T00:00:00Z\",\n",
    "\"2016-06-13T00:00:00Z\",\n",
    "\"2016-06-14T00:00:00Z\",\n",
    "\"2016-06-15T00:00:00Z\",\n",
    "\"2016-06-16T00:00:00Z\",\n",
    "\"2016-06-17T00:00:00Z\",\n",
    "\"2016-06-18T00:00:00Z\",\n",
    "\"2016-06-19T00:00:00Z\",\n",
    "]\n",
    "\n",
    "output_df = pd.DataFrame(id_list ,columns = [\"id\"])\n",
    "\n",
    "for i, day in enumerate(column_list):    \n",
    "    one_day_preduction = result_df[result_df[\"date\"] == date_list[i]]\n",
    "    one_day_preduction = one_day_preduction.rename(columns={'p50': day})\n",
    "    output_df = pd.merge(output_df, one_day_preduction[[\"id\", day]], on=\"id\", how=\"inner\")\n",
    "\n",
    "print(output_df)\n",
    "print(f\"output_df.shape: {output_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    id   F1   F2   F3   F4   F5   F6   F7  \\\n",
      "0          FOODS_3_721_TX_2_validation  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1      HOUSEHOLD_2_225_CA_1_validation  2.0  2.0  1.0  1.0  2.0  2.0  2.0   \n",
      "2          FOODS_1_169_CA_4_validation  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "3        HOBBIES_2_066_WI_2_validation  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "4      HOUSEHOLD_1_137_WI_2_validation  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "...                                ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "30485      FOODS_3_756_TX_3_validation  7.0  6.0  6.0  7.0  7.0  9.0  8.0   \n",
      "30486  HOUSEHOLD_2_069_WI_1_validation  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "30487    HOBBIES_2_079_CA_3_validation  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "30488    HOBBIES_2_140_WI_3_validation  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "30489      FOODS_3_537_WI_2_validation  1.0  1.0  1.0  0.0  1.0  1.0  1.0   \n",
      "\n",
      "        F8   F9  ...  F19  F20  F21  F22  F23  F24  F25  F26  F27  F28  \n",
      "0      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1      2.0  1.0  ...  2.0  2.0  2.0  1.0  2.0  1.0  1.0  1.0  2.0  2.0  \n",
      "2      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4      0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "30485  6.0  7.0  ...  7.0  9.0  8.0  7.0  6.0  5.0  7.0  7.0  8.0  8.0  \n",
      "30486  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "30487  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "30488  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "30489  0.0  1.0  ...  1.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  \n",
      "\n",
      "[30490 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "output_df[\"id\"] = output_df[\"id\"].str.upper()\n",
    "output_df[\"id\"] = output_df[\"id\"].str.replace(\"VALIDATION\", \"validation\")\n",
    "print(output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# file export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent_dir: /home/ec2-user/SageMaker\n",
      "Reading files...\n",
      "Mem. usage decreased to  2.09 Mb (84.5% reduction)\n",
      "sample_submission.csv has 60980 rows and 29 columns\n",
      "my_submission_df:                                   id   F1   F2   F3   F4   F5   F6   F7   F8  \\\n",
      "0      HOBBIES_1_001_CA_1_validation  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
      "1      HOBBIES_1_002_CA_1_validation  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2      HOBBIES_1_003_CA_1_validation  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "3      HOBBIES_1_004_CA_1_validation  1.0  1.0  1.0  1.0  1.0  2.0  3.0  1.0   \n",
      "4      HOBBIES_1_005_CA_1_validation  0.0  1.0  1.0  1.0  1.0  1.0  2.0  0.0   \n",
      "...                              ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "60975    FOODS_3_823_WI_3_evaluation  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "60976    FOODS_3_824_WI_3_evaluation  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "60977    FOODS_3_825_WI_3_evaluation  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "60978    FOODS_3_826_WI_3_evaluation  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "60979    FOODS_3_827_WI_3_evaluation  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "        F9  ...  F19  F20  F21  F22  F23  F24  F25  F26  F27  F28  \n",
      "0      1.0  ...  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1      0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2      0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3      1.0  ...  2.0  2.0  3.0  1.0  1.0  1.0  1.0  1.0  3.0  3.0  \n",
      "4      0.0  ...  1.0  1.0  1.0  0.0  0.0  1.0  0.0  1.0  1.0  1.0  \n",
      "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "60975  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "60976  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "60977  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "60978  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "60979  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[60980 rows x 29 columns]\n",
      "csv data export start\n",
      "csv data export finished. Size: (60980, 29)\n"
     ]
    }
   ],
   "source": [
    "parent_dir = pathlib.Path(os.path.abspath(os.curdir)).parent.parent\n",
    "print(f\"parent_dir: {parent_dir}\")\n",
    "_SAMPLE_SUBMISSION_CSV_FILE = \"sample_submission.csv\"\n",
    "_OUTPUT_DIR = os.path.sep.join([\"data\", \"aws_forecast\"])\n",
    "_DATA_DIR = os.path.sep.join([\"data\", \"M5_Three_shades_of_Dark_Darker_magic\"])\n",
    "\n",
    "# Reading competition sample submission and merging our predictions\n",
    "submission_df = read_csv_data(parent_dir, _SAMPLE_SUBMISSION_CSV_FILE)\n",
    "submission_ids_df = submission_df[[\"id\"]]\n",
    "my_submission_df = submission_ids_df.merge(output_df, on=['id'], how='left').fillna(0)\n",
    "print(f\"my_submission_df: {my_submission_df}\")\n",
    "\n",
    "print(\"csv data export start\")\n",
    "my_submission_df.to_csv(os.path.sep.join([str(parent_dir), _OUTPUT_DIR, _EXPORT_FILE_NAME]), index=False)\n",
    "print('csv data export finished. Size:', my_submission_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
