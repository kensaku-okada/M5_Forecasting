{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://www.kaggle.com/ejunichi/m5-simple-fe from https://www.kaggle.com/ejunichi/m5-three-shades-of-dark-darker-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import gc\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import psutil\n",
    "import warnings\n",
    "\n",
    "# custom import\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from multiprocessing import Pool        # Multiprocess Runs\n",
    "\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constant variables for helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_CORES: 16\n"
     ]
    }
   ],
   "source": [
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "print(f\"N_CORES: {N_CORES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function nicely diplaying a head of Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "def display(*dfs, head=True):\n",
    "    for df in dfs:\n",
    "        IPython.display.display(df.head() if head else df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function fixing random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    \"\"\"Sets seed to make all processes deterministic     # type: int\n",
    "    \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function processing df in multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_df_in_multiprocess(func, t_split):\n",
    "    \"\"\"Process ds in Multiprocess\n",
    "    \n",
    "    \"\"\"\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"メモリ使用量を確認するためのシンプルな「メモリプロファイラ」\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "\n",
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    \"\"\"dtypesを失わないための連結による結合\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  constant variables for data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _DATA_DIR = os.path.sep.join([\"data\", \"M5_Three_shades_of_Dark_Darker_magic\", \"sample\"])\n",
    "_DATA_DIR = os.path.sep.join([\"data\", \"M5_Three_shades_of_Dark_Darker_magic\"])\n",
    "\n",
    "_CALENDAR_CSV_FILE = \"calendar.csv\"\n",
    "_SAMPLE_SUBMISSION_CSV_FILE = \"sample_submission.csv\"\n",
    "_SALES_TRAIN_VALIDATION_CSV_FILE = \"sales_train_validation.csv\"\n",
    "_SELL_PRICES_CSV_FILE = \"sell_prices.csv\"\n",
    "\n",
    "_BASE_GRID_FOR_DARKER_MAGIC = \"base_grid_for_darker_magic.pkl\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    reduce the memory usage of the given dataframe.\n",
    "    https://qiita.com/hiroyuki_kageyama/items/02865616811022f79754\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe\n",
    "        verbose: \n",
    "        \n",
    "    Returns:\n",
    "        df, whose memory usage is reduced.\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns: #columns毎に処理\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def read_data(directory, file_name):\n",
    "    print('Reading files...')\n",
    "    df = pd.read_csv(os.path.sep.join([str(directory), _DATA_DIR, file_name]))\n",
    "    df = reduce_mem_usage(df)\n",
    "    print('{} has {} rows and {} columns'.format(file_name, df.shape[0], df.shape[1]))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_df:                                      id        item_id    dept_id   cat_id  \\\n",
      "0         HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  HOBBIES_1  HOBBIES   \n",
      "1         HOBBIES_1_009_CA_1_validation  HOBBIES_1_009  HOBBIES_1  HOBBIES   \n",
      "2         HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  HOBBIES_1  HOBBIES   \n",
      "3         HOBBIES_1_012_CA_1_validation  HOBBIES_1_012  HOBBIES_1  HOBBIES   \n",
      "4         HOBBIES_1_015_CA_1_validation  HOBBIES_1_015  HOBBIES_1  HOBBIES   \n",
      "...                                 ...            ...        ...      ...   \n",
      "46881672    FOODS_3_823_WI_3_validation    FOODS_3_823    FOODS_3    FOODS   \n",
      "46881673    FOODS_3_824_WI_3_validation    FOODS_3_824    FOODS_3    FOODS   \n",
      "46881674    FOODS_3_825_WI_3_validation    FOODS_3_825    FOODS_3    FOODS   \n",
      "46881675    FOODS_3_826_WI_3_validation    FOODS_3_826    FOODS_3    FOODS   \n",
      "46881676    FOODS_3_827_WI_3_validation    FOODS_3_827    FOODS_3    FOODS   \n",
      "\n",
      "         store_id state_id       d  sales  release  wm_yr_wk  \n",
      "0            CA_1       CA     d_1   12.0        0     11101  \n",
      "1            CA_1       CA     d_1    2.0        0     11101  \n",
      "2            CA_1       CA     d_1    0.0        0     11101  \n",
      "3            CA_1       CA     d_1    0.0        0     11101  \n",
      "4            CA_1       CA     d_1    4.0        0     11101  \n",
      "...           ...      ...     ...    ...      ...       ...  \n",
      "46881672     WI_3       WI  d_1941    NaN        0     11617  \n",
      "46881673     WI_3       WI  d_1941    NaN        0     11617  \n",
      "46881674     WI_3       WI  d_1941    NaN        0     11617  \n",
      "46881675     WI_3       WI  d_1941    NaN      230     11617  \n",
      "46881676     WI_3       WI  d_1941    NaN      304     11617  \n",
      "\n",
      "[46881677 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "parent_dir = pathlib.Path(os.path.abspath(os.curdir)).parent.parent\n",
    "import_file_name = _BASE_GRID_FOR_DARKER_MAGIC\n",
    "\n",
    "grid_df = pd.read_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, import_file_name]))\n",
    "print(f\"grid_df: {grid_df}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constant variables for preprocessing/prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_NUM_IMPORT_ROWS_FOR_MELT: 22257700\n"
     ]
    }
   ],
   "source": [
    "# 予測期間とitem数の定義 / number of items, and number of prediction period\n",
    "_NUM_UNIQUE_ITEM_ID = 30490\n",
    "_DAYS_FOR_PREDICTION = 28\n",
    "\n",
    "DAYS_PER_YEAR = 365\n",
    "_NUM_YEARS_FOR_MELT = 2\n",
    "_NUM_IMPORT_ROWS_FOR_MELT = DAYS_PER_YEAR * _NUM_YEARS_FOR_MELT * _NUM_UNIQUE_ITEM_ID\n",
    "print(f\"_NUM_IMPORT_ROWS_FOR_MELT: {_NUM_IMPORT_ROWS_FOR_MELT}\")\n",
    "\n",
    "_SALES_HISTORY_DAYS = 1913\n",
    "_SALES_HISTORY_START_DAYS_FOR_VALIDATION = _SALES_HISTORY_DAYS + 1\n",
    "_SALES_HISTORY_START_DAYS_FOR_EVALUATION = 1942\n",
    "\n",
    "TARGET = 'sales'\n",
    "MAIN_INDEX = ['id','d']  # We can identify items by these columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# additionally clean grid_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id        item_id    dept_id   cat_id  \\\n",
      "0         HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  HOBBIES_1  HOBBIES   \n",
      "1         HOBBIES_1_009_CA_1_validation  HOBBIES_1_009  HOBBIES_1  HOBBIES   \n",
      "2         HOBBIES_1_010_CA_1_validation  HOBBIES_1_010  HOBBIES_1  HOBBIES   \n",
      "3         HOBBIES_1_012_CA_1_validation  HOBBIES_1_012  HOBBIES_1  HOBBIES   \n",
      "4         HOBBIES_1_015_CA_1_validation  HOBBIES_1_015  HOBBIES_1  HOBBIES   \n",
      "...                                 ...            ...        ...      ...   \n",
      "46881672    FOODS_3_823_WI_3_validation    FOODS_3_823    FOODS_3    FOODS   \n",
      "46881673    FOODS_3_824_WI_3_validation    FOODS_3_824    FOODS_3    FOODS   \n",
      "46881674    FOODS_3_825_WI_3_validation    FOODS_3_825    FOODS_3    FOODS   \n",
      "46881675    FOODS_3_826_WI_3_validation    FOODS_3_826    FOODS_3    FOODS   \n",
      "46881676    FOODS_3_827_WI_3_validation    FOODS_3_827    FOODS_3    FOODS   \n",
      "\n",
      "         store_id state_id     d  sales  release  \n",
      "0            CA_1       CA     1   12.0        0  \n",
      "1            CA_1       CA     1    2.0        0  \n",
      "2            CA_1       CA     1    0.0        0  \n",
      "3            CA_1       CA     1    0.0        0  \n",
      "4            CA_1       CA     1    4.0        0  \n",
      "...           ...      ...   ...    ...      ...  \n",
      "46881672     WI_3       WI  1941    NaN        0  \n",
      "46881673     WI_3       WI  1941    NaN        0  \n",
      "46881674     WI_3       WI  1941    NaN        0  \n",
      "46881675     WI_3       WI  1941    NaN      230  \n",
      "46881676     WI_3       WI  1941    NaN      304  \n",
      "\n",
      "[46881677 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# just remove the prefix \"d_\" from 'd' column\n",
    "grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
    "\n",
    "# 'wm_yr_wk'の削除\n",
    "# テスト値がtrainにないため\n",
    "del grid_df['wm_yr_wk']\n",
    "\n",
    "print(grid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export the cleaned base grid (grid_part_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data export start\n",
      "data export finished. Size: (46881677, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 今後の使用のため（モデルトレーニング）pickleファイルとして保存できます\n",
    "_EXPORT_FILE_NAME = \"clearned_base_grid_for_darker_magic.pkl\"\n",
    "print(\"data export start\")\n",
    "grid_df.to_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, _EXPORT_FILE_NAME]))\n",
    "print('data export finished. Size:', grid_df.shape)\n",
    "\n",
    "del grid_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
