{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://www.kaggle.com/kyakovlev/m5-custom-features from https://www.kaggle.com/ejunichi/m5-three-shades-of-dark-darker-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%sh \n",
    "# conda install -c conda-forge lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import gc\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import psutil\n",
    "import warnings\n",
    "\n",
    "# custom import\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from multiprocessing import Pool        # Multiprocess Runs\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "# warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function fixing random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    \"\"\"Sets seed to make all processes deterministic     # type: int\n",
    "    \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constant variables for helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_CORES: 36\n"
     ]
    }
   ],
   "source": [
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "print(f\"N_CORES: {N_CORES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  constant variables for data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this var according to the dataset you refer to \n",
    "# path to the source's pickle files\n",
    "# _DATA_DIR = os.path.sep.join([\"data\", \"M5_Three_shades_of_Dark_Darker_magic\", \"sample\"])\n",
    "_DATA_DIR = os.path.sep.join([\"data\", \"M5_Three_shades_of_Dark_Darker_magic\"])\n",
    "_OUTPUT_DIR = os.path.sep.join([\"data\", \"M5_Three_shades_of_Dark_Darker_magic\"])\n",
    "\n",
    "_CALENDAR_CSV_FILE = \"calendar.csv\"\n",
    "_SAMPLE_SUBMISSION_CSV_FILE = \"sample_submission.csv\"\n",
    "_SALES_TRAIN_VALIDATION_CSV_FILE = \"sales_train_validation.csv\"\n",
    "_SALES_TRAIN_EVALUATION_CSV_FILE = \"sales_train_evaluation.csv\"\n",
    "_SELL_PRICES_CSV_FILE = \"sell_prices.csv\"\n",
    "\n",
    "#PATHS for Features\n",
    "BASE = \"clearned_base_grid_for_darker_magic_evaluation.pkl\"\n",
    "PRICE = \"base_grid_with_sales_price_features_for_darker_magic_evaluation.pkl\"\n",
    "CALENDAR = \"base_grid_with_calendar_features_for_darker_magic_evaluation.pkl\"\n",
    "LAGS = \"base_grid_with_lag_features_for_28_days_evaluation.pkl\"\n",
    "MEAN_ENC = \"base_grid_with_mean_encoded_ids_means_stds_for_darker_magic_evaluation.pkl\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model hyperparameters and constant variables for training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_DAY_VALIDATION: 1914\n",
      "START_DAY_EVALUATION: 1942\n"
     ]
    }
   ],
   "source": [
    "# 'n_estimators': 1300 may be better\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.03,\n",
    "                    'num_leaves': 2**11-1,\n",
    "                    'min_data_in_leaf': 2**12-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 1400, #特徴量を少し増やしたのでiterationの数も少し増やした。\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': -1,\n",
    "                } \n",
    "# Let's look closer on params\n",
    "\n",
    "## 'boosting_type': 'gbdt'\n",
    "# we have 'goss' option for faster training\n",
    "# but it normally leads to underfit.\n",
    "# Also there is good 'dart' mode\n",
    "# but it takes forever to train\n",
    "# and model performance depends \n",
    "# a lot on random factor \n",
    "# https://www.kaggle.com/c/home-credit-default-risk/discussion/60921\n",
    "\n",
    "## 'objective': 'tweedie'\n",
    "# Tweedie Gradient Boosting for Extremely\n",
    "# Unbalanced Zero-inflated Data\n",
    "# https://arxiv.org/pdf/1811.10192.pdf\n",
    "# and many more articles about tweediie\n",
    "#\n",
    "# Strange (for me) but Tweedie is close in results\n",
    "# to my own ugly loss.\n",
    "# My advice here - make OWN LOSS function\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/140564\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/143070\n",
    "# I think many of you already using it (after poisson kernel appeared) \n",
    "# (kagglers are very good with \"params\" testing and tuning).\n",
    "# Try to figure out why Tweedie works.\n",
    "# probably it will show you new features options\n",
    "# or data transformation (Target transformation?).\n",
    "\n",
    "## 'tweedie_variance_power': 1.1\n",
    "# default = 1.5\n",
    "# set this closer to 2 to shift towards a Gamma distribution\n",
    "# set this closer to 1 to shift towards a Poisson distribution\n",
    "# my CV shows 1.1 is optimal \n",
    "# but you can make your own choice\n",
    "\n",
    "## 'metric': 'rmse'\n",
    "# Doesn't mean anything to us\n",
    "# as competition metric is different\n",
    "# and we don't use early stoppings here.\n",
    "# So rmse serves just for general \n",
    "# model performance overview.\n",
    "# Also we use \"fake\" validation set\n",
    "# (as it makes part of the training set)\n",
    "# so even general rmse score doesn't mean anything))\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834\n",
    "\n",
    "## 'subsample': 0.5\n",
    "# Serves to fight with overfit\n",
    "# this will randomly select part of data without resampling\n",
    "# Chosen by CV (my CV can be wrong!)\n",
    "# Next kernel will be about CV\n",
    "\n",
    "##'subsample_freq': 1\n",
    "# frequency for bagging\n",
    "# default value - seems ok\n",
    "\n",
    "## 'learning_rate': 0.03\n",
    "# Chosen by CV\n",
    "# Smaller - longer training\n",
    "# but there is an option to stop \n",
    "# in \"local minimum\"\n",
    "# Bigger - faster training\n",
    "# but there is a chance to\n",
    "# not find \"global minimum\" minimum\n",
    "\n",
    "## 'num_leaves': 2**11-1\n",
    "## 'min_data_in_leaf': 2**12-1\n",
    "# Force model to use more features\n",
    "# We need it to reduce \"recursive\"\n",
    "# error impact.\n",
    "# Also it leads to overfit\n",
    "# that's why we use small \n",
    "# 'max_bin': 100\n",
    "\n",
    "## l1, l2 regularizations\n",
    "# https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n",
    "# Good tiny explanation\n",
    "# l2 can work with bigger num_leaves\n",
    "# but my CV doesn't show boost\n",
    "                    \n",
    "## 'n_estimators': 1400\n",
    "# CV shows that there should be\n",
    "# different values for each state/store.\n",
    "# Current value was chosen \n",
    "# for general purpose.\n",
    "# As we don't use any early stopings\n",
    "# careful to not overfit Public LB.\n",
    "\n",
    "##'feature_fraction': 0.5\n",
    "# LightGBM will randomly select \n",
    "# part of features on each iteration (tree).\n",
    "# We have maaaany features\n",
    "# and many of them are \"duplicates\"\n",
    "# and many just \"noise\"\n",
    "# good values here - 0.5-0.7 (by CV)\n",
    "\n",
    "## 'boost_from_average': False\n",
    "# There is some \"problem\"\n",
    "# to code boost_from_average for \n",
    "# custom loss\n",
    "# 'True' makes training faster\n",
    "# BUT carefull use it\n",
    "# https://github.com/microsoft/LightGBM/issues/1514\n",
    "# not our case but good to know cons\n",
    "#######################################\n",
    "\n",
    "VER = 3                          # Our model version\n",
    "SEED = 42                        # We want all things to be as deterministic as possible\n",
    "seed_everything(SEED)            \n",
    "lgb_params['seed'] = SEED        \n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'sales'            # Our target column name\n",
    "START_DAY_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
    "END_DAY_TRAIN   = 1913               # End day of our train set\n",
    "PREDICTION_HORIZON_DAYS = 28                 # Prediction horizon\n",
    "_NUM_UNIQUE_ITEM_ID = 30490\n",
    "# Use or not use pretrained models: make this true after completing model training.\n",
    "# USE_AUX = True\n",
    "USE_AUX = False\n",
    "\n",
    "# FEATURES to remove.\n",
    "# These features lead to overfit or values not present in test set\n",
    "REMOVE_FEATURES = ['id','state_id','store_id', 'date','wm_yr_wk','d',TARGET]\n",
    "MEAN_STD_FEATURES   = ['enc_cat_id_mean','enc_cat_id_std',\n",
    "                   'enc_dept_id_mean','enc_dept_id_std',\n",
    "                   'enc_item_id_mean','enc_item_id_std'] \n",
    "\n",
    "# AUX(pretrained) Models paths\n",
    "PRETRAINED_MODEL_DIR = 'trained_model'\n",
    "\n",
    "#SPLITS for lags creation\n",
    "SHIFT_DAYS  = 28\n",
    "N_LAGS     = 15\n",
    "LAGS_SPLIT = [col for col in range(SHIFT_DAYS, SHIFT_DAYS + N_LAGS)]\n",
    "ROLLING_SPLIT = []\n",
    "# original \n",
    "for i in [1,7,14]:\n",
    "    for j in [7,14,30,60]:\n",
    "        ROLLING_SPLIT.append([i,j])\n",
    "        \n",
    "START_DAY_VALIDATION = END_DAY_TRAIN + 1\n",
    "print(f\"START_DAY_VALIDATION: {START_DAY_VALIDATION}\")\n",
    "START_DAY_EVALUATION = START_DAY_VALIDATION + PREDICTION_HORIZON_DAYS\n",
    "print(f\"START_DAY_EVALUATION: {START_DAY_EVALUATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function nicely diplaying a head of Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "def display(*dfs, head=True):\n",
    "    for df in dfs:\n",
    "        IPython.display.display(df.head() if head else df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function processing df in multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_df_in_multiprocess(func, t_split):\n",
    "    \"\"\"Process ds in Multiprocess\n",
    "    \n",
    "    \"\"\"\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    print(f\"num_cores: {num_cores}\")\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"メモリ使用量を確認するためのシンプルな「メモリプロファイラ」\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "\n",
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    \"\"\"\n",
    "    dtypesを失わないための連結による結合\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1\n",
    "\n",
    "\n",
    "def get_base_test():\n",
    "    \"\"\"Recombines Test set after training\n",
    "    \n",
    "    \"\"\"\n",
    "    base_test = pd.DataFrame()\n",
    "\n",
    "    for store_id in STORE_IDS:\n",
    "        test_pkl_path = os.path.sep.join([PRETRAINED_MODEL_DIR, 'test_dataset_'+store_id+'_evaluation.pkl'])\n",
    "        temp_df = pd.read_pickle(test_pkl_path)\n",
    "        temp_df['store_id'] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "    \n",
    "    return base_test\n",
    "\n",
    "\n",
    "\n",
    "##### Helper to make dynamic rolling lags #####\n",
    "def make_lag(lag_day):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'sales_lag_'+str(lag_day)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(lag_day)).astype(np.float16)\n",
    "    return lag_df[[col_name]]\n",
    "\n",
    "\n",
    "def make_lag_roll(lag_day):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    shift_day = lag_day[0]\n",
    "    roll_wind = lag_day[1]\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
    "    return lag_df[[col_name]]\n",
    "##### Helper to make dynamic rolling lags #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    reduce the memory usage of the given dataframe.\n",
    "    https://qiita.com/hiroyuki_kageyama/items/02865616811022f79754\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe\n",
    "        verbose: \n",
    "        \n",
    "    Returns:\n",
    "        df, whose memory usage is reduced.\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns: #columns毎に処理\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def read_csv_data(directory, file_name):\n",
    "    print('Reading files...')\n",
    "    df = pd.read_csv(os.path.sep.join([str(directory), _DATA_DIR, file_name]))\n",
    "    df = reduce_mem_usage(df)\n",
    "    print('{} has {} rows and {} columns'.format(file_name, df.shape[0], df.shape[1]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def read_data_by_store(store):\n",
    "#     # Read and contact basic feature\n",
    "#     df = pd.concat([pd.read_pickle(BASE),\n",
    "#                     pd.read_pickle(PRICE).iloc[:,2:],\n",
    "#                     pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
    "#                     axis=1)\n",
    "\n",
    "    # Read and contact basic feature\n",
    "    parent_dir = pathlib.Path(os.path.abspath(os.curdir)).parent.parent\n",
    "    df = pd.concat([pd.read_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, BASE])),\n",
    "                    pd.read_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, PRICE])).iloc[:,2:],\n",
    "                    pd.read_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, CALENDAR])).iloc[:,2:]],\n",
    "                    axis=1)\n",
    "#     print(f\"df at read_data_by_store: {df}\")\n",
    "    \n",
    "    # Leave only relevant store\n",
    "    df = df[df['store_id']==store]\n",
    "\n",
    "    # With memory limits we have to read lags and mean encoding features separately and drop items that we don't need.\n",
    "    # As our Features Grids are aligned \n",
    "    # we can use index to keep only necessary rows\n",
    "    # Alignment is good for us as concat uses less memory than merge.\n",
    "    df2 = pd.read_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, MEAN_ENC]))[MEAN_STD_FEATURES]\n",
    "    df2 = df2[df2.index.isin(df.index)]\n",
    "    \n",
    "    df3 = pd.read_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, LAGS])).iloc[:,3:]\n",
    "    df3 = df3[df3.index.isin(df.index)]\n",
    "    \n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "    del df2 # to not reach memory limit \n",
    "    \n",
    "    df = pd.concat([df, df3], axis=1)\n",
    "    del df3 # to not reach memory limit \n",
    "    \n",
    "    # Create features list\n",
    "    features = [col for col in list(df) if col not in REMOVE_FEATURES]\n",
    "    df = df[['id','d',TARGET]+features]\n",
    "    \n",
    "    # Skipping first n rows\n",
    "    df = df[df['d']>=START_DAY_TRAIN].reset_index(drop=True)\n",
    "    \n",
    "    return df, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORE_IDS: ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n"
     ]
    }
   ],
   "source": [
    "# parent_dir = pathlib.Path(os.path.abspath(os.curdir)).parent.parent\n",
    "# print(f\"parent_dir: {parent_dir}\")\n",
    "# df_sales_train_validation = read_csv_data(parent_dir, _SALES_TRAIN_VALIDATION_CSV_FILE)\n",
    "# df_sales_train_evaluation = read_csv_data(parent_dir, _SALES_TRAIN_EVALUATION_CSV_FILE)\n",
    "# #STORES ids\n",
    "# STORE_IDS = df_sales_train_validation['store_id']\n",
    "# STORE_IDS = list(STORE_IDS.unique())\n",
    "\n",
    "STORE_IDS = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n",
    "print(f\"STORE_IDS: {STORE_IDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate wrmsse (incomplete. skip as of 20200620)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_df.columns.values: ['id' 'item_id' 'dept_id' 'cat_id' 'store_id' 'state_id' 'd' 'sales'\n",
      " 'release' 'sell_price' 'price_max' 'price_min' 'price_std' 'price_mean'\n",
      " 'price_norm' 'price_nunique' 'item_nunique' 'price_momentum'\n",
      " 'price_momentum_m' 'price_momentum_y' 'event_name_1' 'event_type_1'\n",
      " 'event_name_2' 'event_type_2' 'snap_CA' 'snap_TX' 'snap_WI' 'tm_d' 'tm_w'\n",
      " 'tm_m' 'tm_y' 'tm_wm' 'tm_dw' 'tm_w_end']\n",
      "base_df:                                      id        item_id    dept_id   cat_id  \\\n",
      "0         HOBBIES_1_008_CA_1_evaluation  HOBBIES_1_008  HOBBIES_1  HOBBIES   \n",
      "1         HOBBIES_1_009_CA_1_evaluation  HOBBIES_1_009  HOBBIES_1  HOBBIES   \n",
      "2         HOBBIES_1_010_CA_1_evaluation  HOBBIES_1_010  HOBBIES_1  HOBBIES   \n",
      "3         HOBBIES_1_012_CA_1_evaluation  HOBBIES_1_012  HOBBIES_1  HOBBIES   \n",
      "4         HOBBIES_1_015_CA_1_evaluation  HOBBIES_1_015  HOBBIES_1  HOBBIES   \n",
      "...                                 ...            ...        ...      ...   \n",
      "47735392    FOODS_3_823_WI_3_evaluation    FOODS_3_823    FOODS_3    FOODS   \n",
      "47735393    FOODS_3_824_WI_3_evaluation    FOODS_3_824    FOODS_3    FOODS   \n",
      "47735394    FOODS_3_825_WI_3_evaluation    FOODS_3_825    FOODS_3    FOODS   \n",
      "47735395    FOODS_3_826_WI_3_evaluation    FOODS_3_826    FOODS_3    FOODS   \n",
      "47735396    FOODS_3_827_WI_3_evaluation    FOODS_3_827    FOODS_3    FOODS   \n",
      "\n",
      "         store_id state_id     d  sales  release  sell_price  price_max  \\\n",
      "0            CA_1       CA     1   12.0        0    0.459961   0.500000   \n",
      "1            CA_1       CA     1    2.0        0    1.559570   1.769531   \n",
      "2            CA_1       CA     1    0.0        0    3.169922   3.169922   \n",
      "3            CA_1       CA     1    0.0        0    5.980469   6.519531   \n",
      "4            CA_1       CA     1    4.0        0    0.700195   0.720215   \n",
      "...           ...      ...   ...    ...      ...         ...        ...   \n",
      "47735392     WI_3       WI  1969    NaN        0    2.980469   2.980469   \n",
      "47735393     WI_3       WI  1969    NaN        0    2.480469   2.679688   \n",
      "47735394     WI_3       WI  1969    NaN        0    3.980469   4.378906   \n",
      "47735395     WI_3       WI  1969    NaN      230    1.280273   1.280273   \n",
      "47735396     WI_3       WI  1969    NaN      304    1.000000   1.000000   \n",
      "\n",
      "          price_min  price_std  price_mean  price_norm  price_nunique  \\\n",
      "0          0.419922   0.019791    0.476318    0.919922            4.0   \n",
      "1          1.559570   0.032715    1.764648    0.881348            2.0   \n",
      "2          2.970703   0.046173    2.982422    1.000000            2.0   \n",
      "3          5.980469   0.115906    6.468750    0.917480            3.0   \n",
      "4          0.680176   0.011353    0.707031    0.972168            3.0   \n",
      "...             ...        ...         ...         ...            ...   \n",
      "47735392   2.480469   0.171875    2.802734    1.000000            5.0   \n",
      "47735393   2.000000   0.252930    2.507812    0.925781            4.0   \n",
      "47735394   3.980469   0.187866    4.117188    0.909180            3.0   \n",
      "47735395   1.280273   0.000000    1.280273    1.000000            1.0   \n",
      "47735396   1.000000   0.000000    1.000000    1.000000            1.0   \n",
      "\n",
      "          item_nunique  price_momentum  price_momentum_m  price_momentum_y  \\\n",
      "0                   16             NaN          0.968750          0.949707   \n",
      "1                    9             NaN          0.885742          0.896484   \n",
      "2                   20             NaN          1.064453          1.043945   \n",
      "3                   71             NaN          0.922363          0.959473   \n",
      "4                   16             NaN          0.990234          1.001953   \n",
      "...                ...             ...               ...               ...   \n",
      "47735392           206             1.0          1.032227          1.022461   \n",
      "47735393           135             1.0          0.985840          1.112305   \n",
      "47735394           150             1.0          0.957520          1.000000   \n",
      "47735395            44             1.0          1.000000          1.000000   \n",
      "47735396           142             1.0          1.000000          1.000000   \n",
      "\n",
      "          event_name_1 event_type_1  event_name_2 event_type_2 snap_CA  \\\n",
      "0                  NaN          NaN           NaN          NaN       0   \n",
      "1                  NaN          NaN           NaN          NaN       0   \n",
      "2                  NaN          NaN           NaN          NaN       0   \n",
      "3                  NaN          NaN           NaN          NaN       0   \n",
      "4                  NaN          NaN           NaN          NaN       0   \n",
      "...                ...          ...           ...          ...     ...   \n",
      "47735392  NBAFinalsEnd     Sporting  Father's day     Cultural       0   \n",
      "47735393  NBAFinalsEnd     Sporting  Father's day     Cultural       0   \n",
      "47735394  NBAFinalsEnd     Sporting  Father's day     Cultural       0   \n",
      "47735395  NBAFinalsEnd     Sporting  Father's day     Cultural       0   \n",
      "47735396  NBAFinalsEnd     Sporting  Father's day     Cultural       0   \n",
      "\n",
      "         snap_TX snap_WI  tm_d  tm_w  tm_m  tm_y  tm_wm  tm_dw  tm_w_end  \n",
      "0              0       0    29     4     1     0      5      5         1  \n",
      "1              0       0    29     4     1     0      5      5         1  \n",
      "2              0       0    29     4     1     0      5      5         1  \n",
      "3              0       0    29     4     1     0      5      5         1  \n",
      "4              0       0    29     4     1     0      5      5         1  \n",
      "...          ...     ...   ...   ...   ...   ...    ...    ...       ...  \n",
      "47735392       0       0    19    24     6     5      3      6         1  \n",
      "47735393       0       0    19    24     6     5      3      6         1  \n",
      "47735394       0       0    19    24     6     5      3      6         1  \n",
      "47735395       0       0    19    24     6     5      3      6         1  \n",
      "47735396       0       0    19    24     6     5      3      6         1  \n",
      "\n",
      "[47735397 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read and contact basic feature\n",
    "parent_dir = pathlib.Path(os.path.abspath(os.curdir)).parent.parent\n",
    "base_df = pd.concat([pd.read_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, BASE])),\n",
    "                pd.read_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, PRICE])).iloc[:,2:],\n",
    "                pd.read_pickle(os.path.sep.join([str(parent_dir), _DATA_DIR, CALENDAR])).iloc[:,2:]],\n",
    "                axis=1)\n",
    "\n",
    "print (f\"base_df.columns.values: {base_df.columns.values}\")\n",
    "print(f\"base_df: {base_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測期間とitem数の定義 / number of items, and number of prediction period\n",
    "NUM_ITEMS = 30490\n",
    "DAYS_PRED = 28\n",
    "\n",
    "class WRMSSE(object):\n",
    "    # WRMSSE calculation (source: https://www.kaggle.com/girmdshinsei/for-japanese-beginner-with-wrmsse-in-lgbm)\n",
    "    # LightGBMのMetricとして, WRMSSEの効率的な計算を行う。あくまで, 28day-lagで1つのモデルの予測するときにLGBMで効率的なWRMSSEの計算を行う場合である。\n",
    "\n",
    "    # weight_matという0 or 1の疎行列で、効率的にaggregation levelを行列積で計算出来るようにしている\n",
    "    # LightGBMのMetricを効率的に計算するためにGroupby fucntionを使うことを避けているが、そのため、non-rezo demandのデータを除くと効率的な計算ができない。そのためすべてのitemでnon-zero demand dataとなっている最後の28日分のみで検証するコードとなっている.\n",
    "    # Sparce matrixは順序がProductのItem通りになっていないといけないので注意。\n",
    "\n",
    "    def __init__(self, sales_train_val, base_df):\n",
    "                \n",
    "        self.sales_train_val = sales_train_val\n",
    "        \n",
    "        self.product = sales_train_val[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n",
    "\n",
    "        weight_mat = np.c_[np.ones([NUM_ITEMS,1]).astype(np.int8), # level 1\n",
    "           pd.get_dummies(self.product.state_id.astype(str),drop_first=False).astype('int8').values,\n",
    "           pd.get_dummies(self.product.store_id.astype(str),drop_first=False).astype('int8').values,\n",
    "           pd.get_dummies(self.product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "           pd.get_dummies(self.product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "           pd.get_dummies(self.product.state_id.astype(str) + self.product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "           pd.get_dummies(self.product.state_id.astype(str) + self.product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "           pd.get_dummies(self.product.store_id.astype(str) + self.product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "           pd.get_dummies(self.product.store_id.astype(str) + self.product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "           pd.get_dummies(self.product.item_id.astype(str),drop_first=False).astype('int8').values,\n",
    "           pd.get_dummies(self.product.state_id.astype(str) + self.product.item_id.astype(str),drop_first=False).astype('int8').values,\n",
    "           np.identity(NUM_ITEMS).astype(np.int8) #item :level 12\n",
    "           ].T\n",
    "        self.weight_mat_csr = csr_matrix(weight_mat)\n",
    "        print(f\"self.weight_mat_csr: {self.weight_mat_csr}\")\n",
    "        del weight_mat; gc.collect()\n",
    "        \n",
    "        self.rmsse_demoninator = None\n",
    "        self.weight = None\n",
    "                        \n",
    "    def weight_calc(self):\n",
    "        \"\"\"calculate the denominator of RMSSE, and calculate the weight base on sales amount\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        d_name = ['d_' + str(i+1) for i in range(1913)]\n",
    "\n",
    "        sales_train_val = self.weight_mat_csr * self.sales_train_val[d_name].values\n",
    "        print(f\"sales_train_val: {sales_train_val}\")\n",
    "        print(f\"sales_train_val.shape: {sales_train_val.shape}\")\n",
    "\n",
    "        # calculate the start position(first non-zero demand observed date) for each item / 商品の最初の売上日\n",
    "        # 1-1914のdayの数列のうち, 売上が存在しない日を一旦0にし、0を9999に置換。そのうえでminimum numberを計算\n",
    "        df_tmp = ((sales_train_val>0) * np.tile(np.arange(1,1914),(self.weight_mat_csr.shape[0],1)))\n",
    "\n",
    "        start_no = np.min(np.where(df_tmp==0,9999,df_tmp),axis=1)-1\n",
    "\n",
    "        flag = np.dot(np.diag(1/(start_no+1)) , np.tile(np.arange(1,1914),(self.weight_mat_csr.shape[0],1)))<1\n",
    "\n",
    "        sales_train_val = np.where(flag,np.nan,sales_train_val)\n",
    "\n",
    "        # denominator of RMSSE / RMSSEの分母\n",
    "        rmsse_demoninator = np.nansum(np.diff(sales_train_val,axis=1)**2,axis=1)/(1913-start_no)\n",
    "\n",
    "        # rmsse_demoninator == weight1\n",
    "        self.rmsse_demoninator = rmsse_demoninator\n",
    "\n",
    "#         2016/3/27より前を学習用、2016/3/27~2016/4/24（28day）を検証用として分割\n",
    "#         （LightGBMのEarly stoppingの対象） 交差検証の方法はいろいろと検討余地あり。\n",
    "#         calculate the sales amount for each item/level\n",
    "#         (base_df == data)\n",
    "#     df_tmp = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n",
    "#     df_tmp['amount'] = df_tmp['demand'] * df_tmp['sell_price']\n",
    "#     df_tmp =df_tmp.groupby(['id'])['amount'].apply(np.sum)\n",
    "#     df_tmp = df_tmp[product.id].values\n",
    "#         print(f\"df_tmp: {df_tmp}\")\n",
    "        df_tmp = base_df[(base_df['d'] > END_DAY_TRAIN - PREDICTION_HORIZON_DAYS) & (base_df['d'] <= END_DAY_TRAIN)]\n",
    "        df_tmp['amount'] = df_tmp['sales'] * df_tmp['sell_price']\n",
    "        df_tmp =df_tmp.groupby(['id'])['amount'].apply(np.sum)\n",
    "        df_tmp = df_tmp[self.product.id].values\n",
    "        print(f\"df_tmp: {df_tmp}\")\n",
    "        print(f\"df_tmp.shape: {df_tmp.shape}\")\n",
    "\n",
    "        weight = self.weight_mat_csr * df_tmp \n",
    "\n",
    "        weight = weight/np.sum(weight)\n",
    "\n",
    "        # weight == weight2\n",
    "        self.weight = weight\n",
    "        print(f\"self.weight: {self.weight}\")\n",
    "        print(f\"self.weight.shape: {self.weight.shape}\")\n",
    "        \n",
    "        del sales_train_val\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    def wrmsse(self, preds, data):\n",
    "        \"\"\"calculates for last 28 days to consider the non-zero demand period\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"type(preds): {type(preds)}\")\n",
    "        print(preds.shape)\n",
    "        print(f\"type(data): {type(data)}\")\n",
    "        print(data)\n",
    "        \n",
    "        # actual obserbed values / 正解ラベル\n",
    "        y_true = data.get_label()\n",
    "        print(f\"y_true: {y_true}\")\n",
    "        print(f\"type(y_true): {type(y_true)}\")\n",
    "        print(f\"y_true.shape: {y_true.shape}\")\n",
    "        \n",
    "        y_true = y_true[-(NUM_ITEMS * DAYS_PRED):]\n",
    "        preds = preds[-(NUM_ITEMS * DAYS_PRED):]\n",
    "        # number of columns\n",
    "        num_col = DAYS_PRED\n",
    "        \n",
    "        # reshape data to original array((NUM_ITEMS*num_col,1)->(NUM_ITEMS, num_col) ) / 推論の結果が 1 次元の配列になっているので直す\n",
    "        reshaped_preds = preds.reshape(num_col, NUM_ITEMS).T\n",
    "        reshaped_true = y_true.reshape(num_col, NUM_ITEMS).T\n",
    "            \n",
    "        train = self.weight_mat_csr*np.c_[reshaped_preds, reshaped_true]\n",
    "        \n",
    "        #todo: darker magic に組み込むには、下記スコアの計算を店舗毎にしないといけない。\n",
    "        score = np.sum(\n",
    "                    np.sqrt(\n",
    "                        np.mean(\n",
    "                            np.square(\n",
    "                                train[:,:num_col] - train[:,num_col:])\n",
    "                            ,axis=1) / self.rmsse_demoninator) * self.weight)\n",
    "        \n",
    "        return 'wrmsse', score, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wrmsse = WRMSSE(df_sales_train_validation, base_df)\n",
    "# wrmsse.weight_calc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CA_1\n",
      "features_columns: ['item_id', 'dept_id', 'cat_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'sales_lag_43', 'sales_lag_44', 'sales_lag_45', 'sales_lag_46', 'sales_lag_47', 'sales_lag_48', 'sales_lag_49', 'sales_lag_50', 'sales_lag_51', 'sales_lag_52', 'sales_lag_53', 'sales_lag_54', 'sales_lag_55', 'sales_lag_56', 'sales_lag_57', 'sales_lag_58', 'sales_lag_59', 'sales_lag_365', 'sales_lag_366', 'sales_lag_367', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60']\n",
      "grid_df:                                     id     d  sales        item_id    dept_id  \\\n",
      "0        HOBBIES_1_008_CA_1_evaluation     1   12.0  HOBBIES_1_008  HOBBIES_1   \n",
      "1        HOBBIES_1_009_CA_1_evaluation     1    2.0  HOBBIES_1_009  HOBBIES_1   \n",
      "2        HOBBIES_1_010_CA_1_evaluation     1    0.0  HOBBIES_1_010  HOBBIES_1   \n",
      "3        HOBBIES_1_012_CA_1_evaluation     1    0.0  HOBBIES_1_012  HOBBIES_1   \n",
      "4        HOBBIES_1_015_CA_1_evaluation     1    4.0  HOBBIES_1_015  HOBBIES_1   \n",
      "...                                ...   ...    ...            ...        ...   \n",
      "4873634    FOODS_3_823_CA_1_evaluation  1969    NaN    FOODS_3_823    FOODS_3   \n",
      "4873635    FOODS_3_824_CA_1_evaluation  1969    NaN    FOODS_3_824    FOODS_3   \n",
      "4873636    FOODS_3_825_CA_1_evaluation  1969    NaN    FOODS_3_825    FOODS_3   \n",
      "4873637    FOODS_3_826_CA_1_evaluation  1969    NaN    FOODS_3_826    FOODS_3   \n",
      "4873638    FOODS_3_827_CA_1_evaluation  1969    NaN    FOODS_3_827    FOODS_3   \n",
      "\n",
      "          cat_id  release  sell_price  price_max  price_min  price_std  \\\n",
      "0        HOBBIES        0    0.459961   0.500000   0.419922   0.019791   \n",
      "1        HOBBIES        0    1.559570   1.769531   1.559570   0.032715   \n",
      "2        HOBBIES        0    3.169922   3.169922   2.970703   0.046173   \n",
      "3        HOBBIES        0    5.980469   6.519531   5.980469   0.115906   \n",
      "4        HOBBIES        0    0.700195   0.720215   0.680176   0.011353   \n",
      "...          ...      ...         ...        ...        ...        ...   \n",
      "4873634    FOODS      127    2.980469   2.980469   2.480469   0.152588   \n",
      "4873635    FOODS        0    2.480469   2.679688   2.470703   0.086060   \n",
      "4873636    FOODS        1    3.980469   4.378906   3.980469   0.189087   \n",
      "4873637    FOODS      211    1.280273   1.280273   1.280273   0.000000   \n",
      "4873638    FOODS      403    1.000000   1.000000   1.000000   0.000000   \n",
      "\n",
      "         price_mean  price_norm  price_nunique  item_nunique  price_momentum  \\\n",
      "0          0.476318    0.919922            4.0            16             NaN   \n",
      "1          1.764648    0.881348            2.0             9             NaN   \n",
      "2          2.982422    1.000000            2.0            20             NaN   \n",
      "3          6.468750    0.917480            3.0            71             NaN   \n",
      "4          0.707031    0.972168            3.0            16             NaN   \n",
      "...             ...         ...            ...           ...             ...   \n",
      "4873634    2.755859    1.000000            5.0           236             1.0   \n",
      "4873635    2.630859    0.925781            3.0           138             1.0   \n",
      "4873636    4.117188    0.909180            3.0           165             1.0   \n",
      "4873637    1.280273    1.000000            1.0            36             1.0   \n",
      "4873638    1.000000    1.000000            1.0           137             1.0   \n",
      "\n",
      "         price_momentum_m  price_momentum_y  event_name_1 event_type_1  \\\n",
      "0                0.968750          0.949707           NaN          NaN   \n",
      "1                0.885742          0.896484           NaN          NaN   \n",
      "2                1.064453          1.043945           NaN          NaN   \n",
      "3                0.922363          0.959473           NaN          NaN   \n",
      "4                0.990234          1.001953           NaN          NaN   \n",
      "...                   ...               ...           ...          ...   \n",
      "4873634          1.050781          1.029297  NBAFinalsEnd     Sporting   \n",
      "4873635          0.945801          0.962891  NBAFinalsEnd     Sporting   \n",
      "4873636          0.954102          1.000000  NBAFinalsEnd     Sporting   \n",
      "4873637          1.000000          1.000000  NBAFinalsEnd     Sporting   \n",
      "4873638          1.000000          1.000000  NBAFinalsEnd     Sporting   \n",
      "\n",
      "         event_name_2 event_type_2 snap_CA snap_TX snap_WI  tm_d  tm_w  tm_m  \\\n",
      "0                 NaN          NaN       0       0       0    29     4     1   \n",
      "1                 NaN          NaN       0       0       0    29     4     1   \n",
      "2                 NaN          NaN       0       0       0    29     4     1   \n",
      "3                 NaN          NaN       0       0       0    29     4     1   \n",
      "4                 NaN          NaN       0       0       0    29     4     1   \n",
      "...               ...          ...     ...     ...     ...   ...   ...   ...   \n",
      "4873634  Father's day     Cultural       0       0       0    19    24     6   \n",
      "4873635  Father's day     Cultural       0       0       0    19    24     6   \n",
      "4873636  Father's day     Cultural       0       0       0    19    24     6   \n",
      "4873637  Father's day     Cultural       0       0       0    19    24     6   \n",
      "4873638  Father's day     Cultural       0       0       0    19    24     6   \n",
      "\n",
      "         tm_y  tm_wm  tm_dw  tm_w_end  enc_cat_id_mean  enc_cat_id_std  \\\n",
      "0           0      5      5         1         0.708984        2.251953   \n",
      "1           0      5      5         1         0.708984        2.251953   \n",
      "2           0      5      5         1         0.708984        2.251953   \n",
      "3           0      5      5         1         0.708984        2.251953   \n",
      "4           0      5      5         1         0.708984        2.251953   \n",
      "...       ...    ...    ...       ...              ...             ...   \n",
      "4873634     5      3      6         1         2.107422        5.734375   \n",
      "4873635     5      3      6         1         2.107422        5.734375   \n",
      "4873636     5      3      6         1         2.107422        5.734375   \n",
      "4873637     5      3      6         1         2.107422        5.734375   \n",
      "4873638     5      3      6         1         2.107422        5.734375   \n",
      "\n",
      "         enc_dept_id_mean  enc_dept_id_std  enc_item_id_mean  enc_item_id_std  \\\n",
      "0                0.865234         2.537109          4.683594         7.148438   \n",
      "1                0.865234         2.537109          0.849609         1.754883   \n",
      "2                0.865234         2.537109          0.610352         0.861816   \n",
      "3                0.865234         2.537109          0.381104         0.688965   \n",
      "4                0.865234         2.537109          4.417969         6.679688   \n",
      "...                   ...              ...               ...              ...   \n",
      "4873634          2.619141         7.007812          0.846191         1.739258   \n",
      "4873635          2.619141         7.007812          0.436523         0.947266   \n",
      "4873636          2.619141         7.007812          0.715332         1.207031   \n",
      "4873637          2.619141         7.007812          1.117188         1.478516   \n",
      "4873638          2.619141         7.007812          1.726562         3.132812   \n",
      "\n",
      "         sales_lag_28  sales_lag_29  sales_lag_30  sales_lag_31  sales_lag_32  \\\n",
      "0                 NaN           NaN           NaN           NaN           NaN   \n",
      "1                 NaN           NaN           NaN           NaN           NaN   \n",
      "2                 NaN           NaN           NaN           NaN           NaN   \n",
      "3                 NaN           NaN           NaN           NaN           NaN   \n",
      "4                 NaN           NaN           NaN           NaN           NaN   \n",
      "...               ...           ...           ...           ...           ...   \n",
      "4873634           2.0           4.0           3.0           4.0           2.0   \n",
      "4873635           0.0           0.0           0.0           3.0           0.0   \n",
      "4873636           1.0           0.0           3.0           0.0           2.0   \n",
      "4873637           1.0           2.0           1.0           0.0           3.0   \n",
      "4873638           5.0          10.0           2.0           7.0           5.0   \n",
      "\n",
      "         sales_lag_33  sales_lag_34  sales_lag_35  sales_lag_36  sales_lag_37  \\\n",
      "0                 NaN           NaN           NaN           NaN           NaN   \n",
      "1                 NaN           NaN           NaN           NaN           NaN   \n",
      "2                 NaN           NaN           NaN           NaN           NaN   \n",
      "3                 NaN           NaN           NaN           NaN           NaN   \n",
      "4                 NaN           NaN           NaN           NaN           NaN   \n",
      "...               ...           ...           ...           ...           ...   \n",
      "4873634           0.0           1.0           3.0           0.0           3.0   \n",
      "4873635           2.0           1.0           0.0           0.0           0.0   \n",
      "4873636           0.0           2.0           0.0           0.0           0.0   \n",
      "4873637           3.0           1.0           2.0           1.0           0.0   \n",
      "4873638           1.0           3.0           5.0           3.0          10.0   \n",
      "\n",
      "         sales_lag_38  sales_lag_39  sales_lag_40  sales_lag_41  sales_lag_42  \\\n",
      "0                 NaN           NaN           NaN           NaN           NaN   \n",
      "1                 NaN           NaN           NaN           NaN           NaN   \n",
      "2                 NaN           NaN           NaN           NaN           NaN   \n",
      "3                 NaN           NaN           NaN           NaN           NaN   \n",
      "4                 NaN           NaN           NaN           NaN           NaN   \n",
      "...               ...           ...           ...           ...           ...   \n",
      "4873634           2.0           2.0           2.0           0.0           1.0   \n",
      "4873635           0.0           0.0           3.0           5.0           0.0   \n",
      "4873636           1.0           2.0           1.0           2.0           3.0   \n",
      "4873637           0.0           1.0           3.0           1.0           0.0   \n",
      "4873638           6.0           4.0           0.0           0.0           9.0   \n",
      "\n",
      "         sales_lag_43  sales_lag_44  sales_lag_45  sales_lag_46  sales_lag_47  \\\n",
      "0                 NaN           NaN           NaN           NaN           NaN   \n",
      "1                 NaN           NaN           NaN           NaN           NaN   \n",
      "2                 NaN           NaN           NaN           NaN           NaN   \n",
      "3                 NaN           NaN           NaN           NaN           NaN   \n",
      "4                 NaN           NaN           NaN           NaN           NaN   \n",
      "...               ...           ...           ...           ...           ...   \n",
      "4873634           0.0           2.0           0.0           0.0           1.0   \n",
      "4873635           2.0           0.0           0.0           2.0           2.0   \n",
      "4873636           1.0           1.0           0.0           0.0           4.0   \n",
      "4873637           0.0           2.0           0.0           0.0           4.0   \n",
      "4873638           5.0           8.0           7.0          12.0           5.0   \n",
      "\n",
      "         sales_lag_48  sales_lag_49  sales_lag_50  sales_lag_51  sales_lag_52  \\\n",
      "0                 NaN           NaN           NaN           NaN           NaN   \n",
      "1                 NaN           NaN           NaN           NaN           NaN   \n",
      "2                 NaN           NaN           NaN           NaN           NaN   \n",
      "3                 NaN           NaN           NaN           NaN           NaN   \n",
      "4                 NaN           NaN           NaN           NaN           NaN   \n",
      "...               ...           ...           ...           ...           ...   \n",
      "4873634           3.0           0.0           4.0           1.0           1.0   \n",
      "4873635           3.0           0.0           2.0           0.0           0.0   \n",
      "4873636           1.0           3.0           0.0           2.0           1.0   \n",
      "4873637           0.0           0.0           0.0           1.0           1.0   \n",
      "4873638           4.0           6.0           2.0           1.0           6.0   \n",
      "\n",
      "         sales_lag_53  sales_lag_54  sales_lag_55  sales_lag_56  sales_lag_57  \\\n",
      "0                 NaN           NaN           NaN           NaN           NaN   \n",
      "1                 NaN           NaN           NaN           NaN           NaN   \n",
      "2                 NaN           NaN           NaN           NaN           NaN   \n",
      "3                 NaN           NaN           NaN           NaN           NaN   \n",
      "4                 NaN           NaN           NaN           NaN           NaN   \n",
      "...               ...           ...           ...           ...           ...   \n",
      "4873634           1.0           1.0           0.0           1.0           1.0   \n",
      "4873635           3.0           0.0           1.0           0.0           1.0   \n",
      "4873636           0.0           0.0           2.0           2.0           3.0   \n",
      "4873637           0.0           0.0           0.0           0.0           2.0   \n",
      "4873638           3.0           4.0           1.0          21.0           3.0   \n",
      "\n",
      "         sales_lag_58  sales_lag_59  sales_lag_365  sales_lag_366  \\\n",
      "0                 NaN           NaN            NaN            NaN   \n",
      "1                 NaN           NaN            NaN            NaN   \n",
      "2                 NaN           NaN            NaN            NaN   \n",
      "3                 NaN           NaN            NaN            NaN   \n",
      "4                 NaN           NaN            NaN            NaN   \n",
      "...               ...           ...            ...            ...   \n",
      "4873634           4.0           0.0            3.0            0.0   \n",
      "4873635           2.0           2.0            0.0            0.0   \n",
      "4873636           1.0           0.0            2.0            0.0   \n",
      "4873637           3.0           0.0            1.0            4.0   \n",
      "4873638           5.0           3.0            6.0            4.0   \n",
      "\n",
      "         sales_lag_367  rolling_mean_7  rolling_std_7  rolling_mean_14  \\\n",
      "0                  NaN             NaN            NaN              NaN   \n",
      "1                  NaN             NaN            NaN              NaN   \n",
      "2                  NaN             NaN            NaN              NaN   \n",
      "3                  NaN             NaN            NaN              NaN   \n",
      "4                  NaN             NaN            NaN              NaN   \n",
      "...                ...             ...            ...              ...   \n",
      "4873634            2.0        2.285156       1.496094         2.000000   \n",
      "4873635            0.0        0.856934       1.214844         1.000000   \n",
      "4873636            1.0        1.142578       1.214844         1.000000   \n",
      "4873637            1.0        1.571289       1.133789         1.357422   \n",
      "4873638            5.0        4.714844       3.093750         4.355469   \n",
      "\n",
      "         rolling_std_14  rolling_mean_30  rolling_std_30  rolling_mean_60  \\\n",
      "0                   NaN              NaN             NaN              NaN   \n",
      "1                   NaN              NaN             NaN              NaN   \n",
      "2                   NaN              NaN             NaN              NaN   \n",
      "3                   NaN              NaN             NaN              NaN   \n",
      "4                   NaN              NaN             NaN              NaN   \n",
      "...                 ...              ...             ...              ...   \n",
      "4873634        1.358398         1.500000        1.306641         1.083008   \n",
      "4873635        1.617188         1.000000        1.364258         0.883301   \n",
      "4873636        1.038086         1.233398        1.194336         1.133789   \n",
      "4873637        1.082031         0.966797        1.159180         0.966797   \n",
      "4873638        3.201172         5.265625        4.250000         5.117188   \n",
      "\n",
      "         rolling_std_60  rolling_mean_180  rolling_std_180  \\\n",
      "0                   NaN               NaN              NaN   \n",
      "1                   NaN               NaN              NaN   \n",
      "2                   NaN               NaN              NaN   \n",
      "3                   NaN               NaN              NaN   \n",
      "4                   NaN               NaN              NaN   \n",
      "...                 ...               ...              ...   \n",
      "4873634        1.356445          1.588867         1.838867   \n",
      "4873635        1.462891          0.294434         0.937500   \n",
      "4873636        1.016602          0.950195         1.115234   \n",
      "4873637        1.301758          1.061523         1.415039   \n",
      "4873638        4.125000          4.027344         3.505859   \n",
      "\n",
      "         rolling_mean_tmp_1_7  rolling_mean_tmp_1_14  rolling_mean_tmp_1_30  \\\n",
      "0                         NaN                    NaN                    NaN   \n",
      "1                         NaN                    NaN                    NaN   \n",
      "2                         NaN                    NaN                    NaN   \n",
      "3                         NaN                    NaN                    NaN   \n",
      "4                         NaN                    NaN                    NaN   \n",
      "...                       ...                    ...                    ...   \n",
      "4873634              2.427734               1.928711               1.566406   \n",
      "4873635              0.856934               1.000000               1.066406   \n",
      "4873636              1.000000               1.142578               1.233398   \n",
      "4873637              1.713867               1.286133               1.033203   \n",
      "4873638              4.714844               4.644531               5.265625   \n",
      "\n",
      "         rolling_mean_tmp_1_60  rolling_mean_tmp_7_7  rolling_mean_tmp_7_14  \\\n",
      "0                          NaN                   NaN                    NaN   \n",
      "1                          NaN                   NaN                    NaN   \n",
      "2                          NaN                   NaN                    NaN   \n",
      "3                          NaN                   NaN                    NaN   \n",
      "4                          NaN                   NaN                    NaN   \n",
      "...                        ...                   ...                    ...   \n",
      "4873634               1.049805              1.713867               1.357422   \n",
      "4873635               0.883301              1.142578               1.213867   \n",
      "4873636               1.133789              0.856934               1.142578   \n",
      "4873637               1.000000              1.142578               1.000000   \n",
      "4873638               5.050781              4.000000               5.570312   \n",
      "\n",
      "         rolling_mean_tmp_7_30  rolling_mean_tmp_7_60  rolling_mean_tmp_14_7  \\\n",
      "0                          NaN                    NaN                    NaN   \n",
      "1                          NaN                    NaN                    NaN   \n",
      "2                          NaN                    NaN                    NaN   \n",
      "3                          NaN                    NaN                    NaN   \n",
      "4                          NaN                    NaN                    NaN   \n",
      "...                        ...                    ...                    ...   \n",
      "4873634               1.266602               0.816895               1.000000   \n",
      "4873635               0.966797               0.783203               1.286133   \n",
      "4873636               1.133789               1.083008               1.428711   \n",
      "4873637               0.899902               1.049805               0.856934   \n",
      "4873638               5.800781               5.082031               7.144531   \n",
      "\n",
      "         rolling_mean_tmp_14_14  rolling_mean_tmp_14_30  \\\n",
      "0                           NaN                     NaN   \n",
      "1                           NaN                     NaN   \n",
      "2                           NaN                     NaN   \n",
      "3                           NaN                     NaN   \n",
      "4                           NaN                     NaN   \n",
      "...                         ...                     ...   \n",
      "4873634                1.071289                1.233398   \n",
      "4873635                1.071289                0.766602   \n",
      "4873636                1.286133                1.333008   \n",
      "4873637                0.571289                0.700195   \n",
      "4873638                5.214844                6.265625   \n",
      "\n",
      "         rolling_mean_tmp_14_60  \n",
      "0                           NaN  \n",
      "1                           NaN  \n",
      "2                           NaN  \n",
      "3                           NaN  \n",
      "4                           NaN  \n",
      "...                         ...  \n",
      "4873634                0.616699  \n",
      "4873635                0.649902  \n",
      "4873636                1.133789  \n",
      "4873637                1.049805  \n",
      "4873638                5.035156  \n",
      "\n",
      "[4873639 rows x 95 columns]\n",
      "train_mask.shape: (4873639,)\n",
      "valid_mask.shape: (4873639,)\n",
      "preds_mask.shape: (4873639,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_df[train_mask][features_columns].shape: (4702895, 92)\n",
      "grid_df[valid_mask][features_columns].shape: (4873639, 92)\n",
      "valid_data: <lightgbm.basic.Dataset object at 0x7f5e0b240978>\n",
      "grid_df after grid_df[preds_mask].reset_index(drop=True):                                   id     d  sales        item_id    dept_id  \\\n",
      "0      HOBBIES_1_001_CA_1_evaluation  1942    NaN  HOBBIES_1_001  HOBBIES_1   \n",
      "1      HOBBIES_1_002_CA_1_evaluation  1942    NaN  HOBBIES_1_002  HOBBIES_1   \n",
      "2      HOBBIES_1_003_CA_1_evaluation  1942    NaN  HOBBIES_1_003  HOBBIES_1   \n",
      "3      HOBBIES_1_004_CA_1_evaluation  1942    NaN  HOBBIES_1_004  HOBBIES_1   \n",
      "4      HOBBIES_1_005_CA_1_evaluation  1942    NaN  HOBBIES_1_005  HOBBIES_1   \n",
      "...                              ...   ...    ...            ...        ...   \n",
      "85367    FOODS_3_823_CA_1_evaluation  1969    NaN    FOODS_3_823    FOODS_3   \n",
      "85368    FOODS_3_824_CA_1_evaluation  1969    NaN    FOODS_3_824    FOODS_3   \n",
      "85369    FOODS_3_825_CA_1_evaluation  1969    NaN    FOODS_3_825    FOODS_3   \n",
      "85370    FOODS_3_826_CA_1_evaluation  1969    NaN    FOODS_3_826    FOODS_3   \n",
      "85371    FOODS_3_827_CA_1_evaluation  1969    NaN    FOODS_3_827    FOODS_3   \n",
      "\n",
      "        cat_id  release  sell_price  price_max  price_min  price_std  \\\n",
      "0      HOBBIES      224    8.382812   9.578125   8.257812   0.152344   \n",
      "1      HOBBIES       20    3.970703   3.970703   3.970703   0.000000   \n",
      "2      HOBBIES      300    2.970703   2.970703   2.970703   0.000000   \n",
      "3      HOBBIES        5    4.640625   4.640625   4.339844   0.145630   \n",
      "4      HOBBIES       16    2.880859   3.080078   2.480469   0.150024   \n",
      "...        ...      ...         ...        ...        ...        ...   \n",
      "85367    FOODS      127    2.980469   2.980469   2.480469   0.152588   \n",
      "85368    FOODS        0    2.480469   2.679688   2.470703   0.086060   \n",
      "85369    FOODS        1    3.980469   4.378906   3.980469   0.189087   \n",
      "85370    FOODS      211    1.280273   1.280273   1.280273   0.000000   \n",
      "85371    FOODS      403    1.000000   1.000000   1.000000   0.000000   \n",
      "\n",
      "       price_mean  price_norm  price_nunique  item_nunique  price_momentum  \\\n",
      "0        8.281250    0.875000            3.0             7             1.0   \n",
      "1        3.970703    1.000000            1.0           131             1.0   \n",
      "2        2.970703    1.000000            1.0           118             1.0   \n",
      "3        4.527344    1.000000            2.0             2             1.0   \n",
      "4        2.941406    0.935547            4.0           161             1.0   \n",
      "...           ...         ...            ...           ...             ...   \n",
      "85367    2.755859    1.000000            5.0           236             1.0   \n",
      "85368    2.630859    0.925781            3.0           138             1.0   \n",
      "85369    4.117188    0.909180            3.0           165             1.0   \n",
      "85370    1.280273    1.000000            1.0            36             1.0   \n",
      "85371    1.000000    1.000000            1.0           137             1.0   \n",
      "\n",
      "       price_momentum_m  price_momentum_y  event_name_1 event_type_1  \\\n",
      "0              1.010742          1.008789           NaN          NaN   \n",
      "1              1.000000          1.000000           NaN          NaN   \n",
      "2              1.000000          1.000000           NaN          NaN   \n",
      "3              1.020508          1.000000           NaN          NaN   \n",
      "4              0.966797          1.000000           NaN          NaN   \n",
      "...                 ...               ...           ...          ...   \n",
      "85367          1.050781          1.029297  NBAFinalsEnd     Sporting   \n",
      "85368          0.945801          0.962891  NBAFinalsEnd     Sporting   \n",
      "85369          0.954102          1.000000  NBAFinalsEnd     Sporting   \n",
      "85370          1.000000          1.000000  NBAFinalsEnd     Sporting   \n",
      "85371          1.000000          1.000000  NBAFinalsEnd     Sporting   \n",
      "\n",
      "       event_name_2 event_type_2 snap_CA snap_TX snap_WI  tm_d  tm_w  tm_m  \\\n",
      "0               NaN          NaN       0       0       0    23    21     5   \n",
      "1               NaN          NaN       0       0       0    23    21     5   \n",
      "2               NaN          NaN       0       0       0    23    21     5   \n",
      "3               NaN          NaN       0       0       0    23    21     5   \n",
      "4               NaN          NaN       0       0       0    23    21     5   \n",
      "...             ...          ...     ...     ...     ...   ...   ...   ...   \n",
      "85367  Father's day     Cultural       0       0       0    19    24     6   \n",
      "85368  Father's day     Cultural       0       0       0    19    24     6   \n",
      "85369  Father's day     Cultural       0       0       0    19    24     6   \n",
      "85370  Father's day     Cultural       0       0       0    19    24     6   \n",
      "85371  Father's day     Cultural       0       0       0    19    24     6   \n",
      "\n",
      "       tm_y  tm_wm  tm_dw  tm_w_end  enc_cat_id_mean  enc_cat_id_std  \\\n",
      "0         5      4      0         0         0.708984        2.251953   \n",
      "1         5      4      0         0         0.708984        2.251953   \n",
      "2         5      4      0         0         0.708984        2.251953   \n",
      "3         5      4      0         0         0.708984        2.251953   \n",
      "4         5      4      0         0         0.708984        2.251953   \n",
      "...     ...    ...    ...       ...              ...             ...   \n",
      "85367     5      3      6         1         2.107422        5.734375   \n",
      "85368     5      3      6         1         2.107422        5.734375   \n",
      "85369     5      3      6         1         2.107422        5.734375   \n",
      "85370     5      3      6         1         2.107422        5.734375   \n",
      "85371     5      3      6         1         2.107422        5.734375   \n",
      "\n",
      "       enc_dept_id_mean  enc_dept_id_std  enc_item_id_mean  enc_item_id_std  \\\n",
      "0              0.865234         2.537109          0.411133         0.750000   \n",
      "1              0.865234         2.537109          0.273438         0.601562   \n",
      "2              0.865234         2.537109          0.188232         0.491455   \n",
      "3              0.865234         2.537109          2.058594         2.667969   \n",
      "4              0.865234         2.537109          0.813477         1.245117   \n",
      "...                 ...              ...               ...              ...   \n",
      "85367          2.619141         7.007812          0.846191         1.739258   \n",
      "85368          2.619141         7.007812          0.436523         0.947266   \n",
      "85369          2.619141         7.007812          0.715332         1.207031   \n",
      "85370          2.619141         7.007812          1.117188         1.478516   \n",
      "85371          2.619141         7.007812          1.726562         3.132812   \n",
      "\n",
      "       sales_lag_28  sales_lag_29  sales_lag_30  sales_lag_31  sales_lag_32  \\\n",
      "0               0.0           1.0           1.0           0.0           3.0   \n",
      "1               0.0           0.0           0.0           0.0           0.0   \n",
      "2               0.0           1.0           1.0           1.0           0.0   \n",
      "3               0.0           2.0           7.0           3.0           1.0   \n",
      "4               1.0           4.0           2.0           2.0           2.0   \n",
      "...             ...           ...           ...           ...           ...   \n",
      "85367           2.0           4.0           3.0           4.0           2.0   \n",
      "85368           0.0           0.0           0.0           3.0           0.0   \n",
      "85369           1.0           0.0           3.0           0.0           2.0   \n",
      "85370           1.0           2.0           1.0           0.0           3.0   \n",
      "85371           5.0          10.0           2.0           7.0           5.0   \n",
      "\n",
      "       sales_lag_33  sales_lag_34  sales_lag_35  sales_lag_36  sales_lag_37  \\\n",
      "0               1.0           1.0           1.0           0.0           3.0   \n",
      "1               1.0           0.0           0.0           0.0           0.0   \n",
      "2               1.0           1.0           1.0           2.0           1.0   \n",
      "3               0.0           1.0           4.0           5.0           0.0   \n",
      "4               1.0           1.0           0.0           1.0           1.0   \n",
      "...             ...           ...           ...           ...           ...   \n",
      "85367           0.0           1.0           3.0           0.0           3.0   \n",
      "85368           2.0           1.0           0.0           0.0           0.0   \n",
      "85369           0.0           2.0           0.0           0.0           0.0   \n",
      "85370           3.0           1.0           2.0           1.0           0.0   \n",
      "85371           1.0           3.0           5.0           3.0          10.0   \n",
      "\n",
      "       sales_lag_38  sales_lag_39  sales_lag_40  sales_lag_41  sales_lag_42  \\\n",
      "0               1.0           1.0           0.0           0.0           0.0   \n",
      "1               0.0           0.0           0.0           0.0           0.0   \n",
      "2               2.0           2.0           1.0           0.0           0.0   \n",
      "3               1.0           0.0           3.0           0.0           2.0   \n",
      "4               2.0           1.0           1.0           0.0           2.0   \n",
      "...             ...           ...           ...           ...           ...   \n",
      "85367           2.0           2.0           2.0           0.0           1.0   \n",
      "85368           0.0           0.0           3.0           5.0           0.0   \n",
      "85369           1.0           2.0           1.0           2.0           3.0   \n",
      "85370           0.0           1.0           3.0           1.0           0.0   \n",
      "85371           6.0           4.0           0.0           0.0           9.0   \n",
      "\n",
      "       sales_lag_43  sales_lag_44  sales_lag_45  sales_lag_46  sales_lag_47  \\\n",
      "0               2.0           1.0           0.0           3.0           2.0   \n",
      "1               0.0           0.0           0.0           0.0           0.0   \n",
      "2               0.0           0.0           0.0           1.0           0.0   \n",
      "3               4.0           5.0           2.0           0.0           1.0   \n",
      "4               1.0           1.0           0.0           1.0           0.0   \n",
      "...             ...           ...           ...           ...           ...   \n",
      "85367           0.0           2.0           0.0           0.0           1.0   \n",
      "85368           2.0           0.0           0.0           2.0           2.0   \n",
      "85369           1.0           1.0           0.0           0.0           4.0   \n",
      "85370           0.0           2.0           0.0           0.0           4.0   \n",
      "85371           5.0           8.0           7.0          12.0           5.0   \n",
      "\n",
      "       sales_lag_48  sales_lag_49  sales_lag_50  sales_lag_51  sales_lag_52  \\\n",
      "0               4.0           0.0           1.0           0.0           0.0   \n",
      "1               0.0           0.0           0.0           0.0           0.0   \n",
      "2               0.0           0.0           1.0           0.0           0.0   \n",
      "3               3.0           1.0           2.0           1.0           3.0   \n",
      "4               1.0           0.0           4.0           1.0           0.0   \n",
      "...             ...           ...           ...           ...           ...   \n",
      "85367           3.0           0.0           4.0           1.0           1.0   \n",
      "85368           3.0           0.0           2.0           0.0           0.0   \n",
      "85369           1.0           3.0           0.0           2.0           1.0   \n",
      "85370           0.0           0.0           0.0           1.0           1.0   \n",
      "85371           4.0           6.0           2.0           1.0           6.0   \n",
      "\n",
      "       sales_lag_53  sales_lag_54  sales_lag_55  sales_lag_56  sales_lag_57  \\\n",
      "0               0.0           0.0           0.0           1.0           1.0   \n",
      "1               0.0           0.0           0.0           1.0           1.0   \n",
      "2               0.0           0.0           0.0           0.0           0.0   \n",
      "3               0.0           0.0           0.0           0.0           6.0   \n",
      "4               4.0           4.0           0.0           1.0           0.0   \n",
      "...             ...           ...           ...           ...           ...   \n",
      "85367           1.0           1.0           0.0           1.0           1.0   \n",
      "85368           3.0           0.0           1.0           0.0           1.0   \n",
      "85369           0.0           0.0           2.0           2.0           3.0   \n",
      "85370           0.0           0.0           0.0           0.0           2.0   \n",
      "85371           3.0           4.0           1.0          21.0           3.0   \n",
      "\n",
      "       sales_lag_58  sales_lag_59  sales_lag_365  sales_lag_366  \\\n",
      "0               1.0           1.0            0.0            0.0   \n",
      "1               1.0           1.0            0.0            0.0   \n",
      "2               1.0           1.0            0.0            1.0   \n",
      "3               6.0           0.0            2.0            4.0   \n",
      "4               0.0           0.0            3.0            0.0   \n",
      "...             ...           ...            ...            ...   \n",
      "85367           4.0           0.0            3.0            0.0   \n",
      "85368           2.0           2.0            0.0            0.0   \n",
      "85369           1.0           0.0            2.0            0.0   \n",
      "85370           3.0           0.0            1.0            4.0   \n",
      "85371           5.0           3.0            6.0            4.0   \n",
      "\n",
      "       sales_lag_367  rolling_mean_7  rolling_std_7  rolling_mean_14  \\\n",
      "0                0.0        1.000000       1.000000         0.928711   \n",
      "1                0.0        0.142822       0.377930         0.071411   \n",
      "2                0.0        0.714355       0.488037         1.000000   \n",
      "3                3.0        2.000000       2.449219         1.928711   \n",
      "4                0.0        1.857422       1.069336         1.357422   \n",
      "...              ...             ...            ...              ...   \n",
      "85367            2.0        2.285156       1.496094         2.000000   \n",
      "85368            0.0        0.856934       1.214844         1.000000   \n",
      "85369            1.0        1.142578       1.214844         1.000000   \n",
      "85370            1.0        1.571289       1.133789         1.357422   \n",
      "85371            5.0        4.714844       3.093750         4.355469   \n",
      "\n",
      "       rolling_std_14  rolling_mean_30  rolling_std_30  rolling_mean_60  \\\n",
      "0            0.997070         0.933105        1.112305         1.033203   \n",
      "1            0.267334         0.099976        0.305176         0.183350   \n",
      "2            0.679199         0.533203        0.681641         0.516602   \n",
      "3            2.199219         1.900391        2.005859         1.883789   \n",
      "4            1.007812         1.299805        1.263672         1.216797   \n",
      "...               ...              ...             ...              ...   \n",
      "85367        1.358398         1.500000        1.306641         1.083008   \n",
      "85368        1.617188         1.000000        1.364258         0.883301   \n",
      "85369        1.038086         1.233398        1.194336         1.133789   \n",
      "85370        1.082031         0.966797        1.159180         0.966797   \n",
      "85371        3.201172         5.265625        4.250000         5.117188   \n",
      "\n",
      "       rolling_std_60  rolling_mean_180  rolling_std_180  \\\n",
      "0            1.088867          0.827637         1.050781   \n",
      "1            0.390137          0.338867         0.741211   \n",
      "2            0.947754          0.644531         1.000977   \n",
      "3            1.958008          1.844727         1.749023   \n",
      "4            1.106445          1.155273         1.200195   \n",
      "...               ...               ...              ...   \n",
      "85367        1.356445          1.588867         1.838867   \n",
      "85368        1.462891          0.294434         0.937500   \n",
      "85369        1.016602          0.950195         1.115234   \n",
      "85370        1.301758          1.061523         1.415039   \n",
      "85371        4.125000          4.027344         3.505859   \n",
      "\n",
      "       rolling_mean_tmp_1_7  rolling_mean_tmp_1_14  rolling_mean_tmp_1_30  \\\n",
      "0                  1.142578               0.928711               0.966797   \n",
      "1                  0.142822               0.071411               0.133301   \n",
      "2                  0.856934               1.000000               0.566895   \n",
      "3                  2.572266               2.072266               2.099609   \n",
      "4                  1.713867               1.428711               1.266602   \n",
      "...                     ...                    ...                    ...   \n",
      "85367              2.427734               1.928711               1.566406   \n",
      "85368              0.856934               1.000000               1.066406   \n",
      "85369              1.000000               1.142578               1.233398   \n",
      "85370              1.713867               1.286133               1.033203   \n",
      "85371              4.714844               4.644531               5.265625   \n",
      "\n",
      "       rolling_mean_tmp_1_60  rolling_mean_tmp_7_7  rolling_mean_tmp_7_14  \\\n",
      "0                   1.033203              0.856934               1.286133   \n",
      "1                   0.199951              0.000000               0.000000   \n",
      "2                   0.516602              1.286133               0.714355   \n",
      "3                   1.916992              1.857422               2.142578   \n",
      "4                   1.250000              0.856934               0.856934   \n",
      "...                      ...                   ...                    ...   \n",
      "85367               1.049805              1.713867               1.357422   \n",
      "85368               0.883301              1.142578               1.213867   \n",
      "85369               1.133789              0.856934               1.142578   \n",
      "85370               1.000000              1.142578               1.000000   \n",
      "85371               5.050781              4.000000               5.570312   \n",
      "\n",
      "       rolling_mean_tmp_7_30  rolling_mean_tmp_7_60  rolling_mean_tmp_14_7  \\\n",
      "0                   1.033203               1.049805               1.713867   \n",
      "1                   0.166626               0.199951               0.000000   \n",
      "2                   0.433350               0.466553               0.142822   \n",
      "3                   2.099609               1.900391               2.427734   \n",
      "4                   1.133789               1.099609               0.856934   \n",
      "...                      ...                    ...                    ...   \n",
      "85367               1.266602               0.816895               1.000000   \n",
      "85368               0.966797               0.783203               1.286133   \n",
      "85369               1.133789               1.083008               1.428711   \n",
      "85370               0.899902               1.049805               0.856934   \n",
      "85371               5.800781               5.082031               7.144531   \n",
      "\n",
      "       rolling_mean_tmp_14_14  rolling_mean_tmp_14_30  rolling_mean_tmp_14_60  \n",
      "0                    0.928711                1.066406                1.049805  \n",
      "1                    0.000000                0.166626                0.216675  \n",
      "2                    0.142822                0.199951                0.366699  \n",
      "3                    1.713867                2.166016                1.816406  \n",
      "4                    1.357422                1.266602                1.200195  \n",
      "...                       ...                     ...                     ...  \n",
      "85367                1.071289                1.233398                0.616699  \n",
      "85368                1.071289                0.766602                0.649902  \n",
      "85369                1.286133                1.333008                1.133789  \n",
      "85370                0.571289                0.700195                1.049805  \n",
      "85371                5.214844                6.265625                5.035156  \n",
      "\n",
      "[85372 rows x 95 columns]\n",
      "keep_cols: ['id', 'd', 'sales', 'item_id', 'dept_id', 'cat_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'sales_lag_43', 'sales_lag_44', 'sales_lag_45', 'sales_lag_46', 'sales_lag_47', 'sales_lag_48', 'sales_lag_49', 'sales_lag_50', 'sales_lag_51', 'sales_lag_52', 'sales_lag_53', 'sales_lag_54', 'sales_lag_55', 'sales_lag_56', 'sales_lag_57', 'sales_lag_58', 'sales_lag_59', 'sales_lag_365', 'sales_lag_366', 'sales_lag_367', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_180', 'rolling_std_180']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 2.84244\n",
      "[200]\tvalid_0's rmse: 2.71928\n",
      "[300]\tvalid_0's rmse: 2.66216\n",
      "[400]\tvalid_0's rmse: 2.62056\n",
      "[500]\tvalid_0's rmse: 2.58897\n",
      "[600]\tvalid_0's rmse: 2.56452\n",
      "[700]\tvalid_0's rmse: 2.54315\n",
      "[800]\tvalid_0's rmse: 2.52292\n",
      "[900]\tvalid_0's rmse: 2.50563\n",
      "[1000]\tvalid_0's rmse: 2.48971\n",
      "[1100]\tvalid_0's rmse: 2.47524\n",
      "[1200]\tvalid_0's rmse: 2.46085\n",
      "[1300]\tvalid_0's rmse: 2.44807\n",
      "[1400]\tvalid_0's rmse: 2.43628\n",
      "[1500]\tvalid_0's rmse: 2.42375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:69: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_60', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_60'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-80803b90eee3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m#         show feature importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mimportance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'importance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"importance.sort_values('importance',ascending=False): {importance.sort_values('importance',ascending=False)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2804\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2806\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m         )\n\u001b[1;32m   1555\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1644\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"loc\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_60', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_60'] not in index\""
     ]
    }
   ],
   "source": [
    "if not USE_AUX:\n",
    "    for store_id in STORE_IDS:\n",
    "        print('Train', store_id)\n",
    "\n",
    "        # Get grid for current store\n",
    "        grid_df, features_columns = read_data_by_store(store_id)\n",
    "        print(f\"features_columns: {features_columns}\")\n",
    "        print(f\"grid_df: {grid_df}\")\n",
    "\n",
    "#         Mask for each stage：\n",
    "#                 1~1913 = training\n",
    "#                 1914~1941= validation, \n",
    "#                 1942~1969 = prediction\n",
    "        train_mask = grid_df['d']<=END_DAY_TRAIN\n",
    "        print(f\"train_mask.shape: {train_mask.shape}\")\n",
    "        valid_mask = grid_df['d']>=(START_DAY_VALIDATION) & (grid_df['d']<START_DAY_EVALUATION)\n",
    "        print(f\"valid_mask.shape: {valid_mask.shape}\")\n",
    "        preds_mask = grid_df['d']>=START_DAY_EVALUATION\n",
    "        print(f\"preds_mask.shape: {preds_mask.shape}\")\n",
    "        \n",
    "        # Apply masks and save lgb dataset as bin to reduce memory spikes during dtype conversions\n",
    "        # https://github.com/Microsoft/LightGBM/issues/1032\n",
    "        # \"To avoid any conversions, you should always use np.float32\" or save to bin before start training\n",
    "        # https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/53773\n",
    "\n",
    "        print(f\"grid_df[train_mask][features_columns].shape: {grid_df[train_mask][features_columns].shape}\")        \n",
    "#         print(f\"grid_df[train_mask][features_columns]: {grid_df[train_mask][features_columns]}\")\n",
    "        train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n",
    "                           label=grid_df[train_mask][TARGET])\n",
    "        train_data.save_binary('training_lgb_dataset_evaluation.bin')\n",
    "        train_data = lgb.Dataset('training_lgb_dataset_evaluation.bin')\n",
    "\n",
    "        print(f\"grid_df[valid_mask][features_columns].shape: {grid_df[valid_mask][features_columns].shape}\")\n",
    "#         print(f\"grid_df[valid_mask][features_columns]: {grid_df[valid_mask][features_columns]}\")\n",
    "        valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
    "                           label=grid_df[valid_mask][TARGET])\n",
    "        print(f\"valid_data: {valid_data}\")\n",
    "\n",
    "        # Saving part of the dataset for later predictions\n",
    "        grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "        print(f\"grid_df after grid_df[preds_mask].reset_index(drop=True): {grid_df}\")\n",
    "        # Removing features that we need to calculate recursively (feature_engineering_for_lag_features.ipynbのrollingによって生成されたFeatureがここで除かれる。後ほど再計算される(leakageを防ぐため))\n",
    "        keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "        print(f\"keep_cols: {keep_cols}\")\n",
    "        grid_df = grid_df[keep_cols]\n",
    "        grid_df.to_pickle(os.path.sep.join([PRETRAINED_MODEL_DIR, 'test_dataset_'+store_id+'_evaluation.pkl']))    \n",
    "\n",
    "        # Launch seeder again to make lgb training 100% deterministic with each \"code line\" np.random \"evolves\" \n",
    "        # so we need (may want) to \"reset\" it\n",
    "        seed_everything(SEED)\n",
    "    #### original hyperparameters #####\n",
    "        estimator = lgb.train(lgb_params,\n",
    "                              train_data,\n",
    "                              valid_sets = [valid_data],\n",
    "                              verbose_eval = 100,\n",
    "                              )\n",
    "    ###########################\n",
    "#     ##### my hyperparameters #####\n",
    "#         estimator = lgb.train(params=lgb_params,\n",
    "#                               train_set=train_data,\n",
    "#                               valid_sets = [train_data, valid_data],\n",
    "#                               verbose_eval = 100,\n",
    "#                               early_stopping_rounds = 200,\n",
    "#                               feval= wrmsse.wrmsse\n",
    "#                               )\n",
    "#     ############################\n",
    "\n",
    "#         show feature importance \n",
    "        importance = pd.DataFrame(estimator.feature_importance(), index=grid_df[train_mask][features_columns].columns, columns=['importance'])\n",
    "        print(f\"importance.sort_values('importance',ascending=False): {importance.sort_values('importance',ascending=False)}\")\n",
    "\n",
    "        del grid_df\n",
    "        gc.collect()\n",
    "\n",
    "              \n",
    "        # Save model - it's not real '.bin' but a pickle file\n",
    "        # estimator = lgb.Booster(model_file='model.txt') can only predict with the best iteration (or the saving iteration)\n",
    "        # pickle.dump gives us more flexibility like estimator.predict(TEST, num_iteration=100)\n",
    "        # num_iteration - number of iteration you want to predict with, \n",
    "        # NULL or <= 0 means use best iteration\n",
    "        model_name = 'lgb_model_'+store_id+'_v'+str(VER)+'_evaluation.bin'\n",
    "#         pickle.dump(estimator, open(model_name, 'wb'))\n",
    "        pickle.dump(estimator, open(os.path.sep.join([PRETRAINED_MODEL_DIR, model_name]), 'wb'))\n",
    "\n",
    "        # Remove temporary files and objects to free some disk space and ram memory\n",
    "        !rm training_lgb_dataset_evaluation.bin\n",
    "        del train_data, valid_data, estimator\n",
    "        gc.collect()\n",
    "else:\n",
    "    # If we want to use pretrained models we can skip training \n",
    "    store_id = STORE_IDS[0]\n",
    "    print(f\"store_id: {store_id}\")\n",
    "\n",
    "    # we just want the column name list\n",
    "    _, features_columns = read_data_by_store(store_id)\n",
    "    print(f\"features_columns: {features_columns}\")\n",
    "    print(f\"len(features_columns): {len(features_columns)}\")\n",
    "    \n",
    "# \"Keep\" models features for predictions\n",
    "MODEL_FEATURES = features_columns\n",
    "print(f\"len(features_columns): {len(features_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict the target column with evaluation days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummy DataFrame to store predictions\n",
    "all_preds = pd.DataFrame()\n",
    "\n",
    "# Join back the Test dataset with a small part of the training data to make recursive features\n",
    "base_test = get_base_test()\n",
    "\n",
    "# Timer to measure predictions time \n",
    "main_time = time.time()\n",
    "\n",
    "# Loop over each prediction day\n",
    "# As rolling lags are the most timeconsuming, we will calculate it for whole day\n",
    "for PREDICT_DAY in range(1, SHIFT_DAYS + 1):    \n",
    "    print('Predict | Day:', PREDICT_DAY)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Make temporary grid to calculate rolling lags\n",
    "    grid_df = base_test.copy()\n",
    "    \n",
    "    print(f\"grid_df: {grid_df}\")\n",
    "    print(f\"grid_df.shape: {grid_df.shape}\")\n",
    "    \n",
    "    print(f\"ROLLING_SPLIT: {ROLLING_SPLIT}\")\n",
    "#     keep_cols = [col for col in list(grid_df) if '_tmp_' not in col] で除外したRollingを再計算して追加(leakageを防ぐため)\n",
    "    grid_df = pd.concat([grid_df, run_df_in_multiprocess(make_lag_roll, ROLLING_SPLIT)], axis=1)\n",
    "        \n",
    "    for store_id in STORE_IDS:\n",
    "        # Read all our models and make predictions for each day/store pairs\n",
    "        model_path = 'lgb_model_'+store_id+'_v'+str(VER)+'_evaluation.bin' \n",
    "#             model_path = PRETRAINED_MODEL_DIR + model_path\n",
    "        model_path = os.path.sep.join([PRETRAINED_MODEL_DIR, model_path])\n",
    "                   \n",
    "        estimator = pickle.load(open(model_path, 'rb'))\n",
    "\n",
    "        day_mask = base_test['d']==(START_DAY_EVALUATION -1 + PREDICT_DAY)\n",
    "#         print(f\"day_mask: {day_mask}\")\n",
    "        store_mask = base_test['store_id']==store_id\n",
    "#         print(f\"store_mask: {store_mask}\")\n",
    "        \n",
    "        mask = (day_mask)&(store_mask)\n",
    "#         print(f\"mask: {mask}\")\n",
    "\n",
    "        print(f\"grid_df[mask][MODEL_FEATURES]: {grid_df[mask][MODEL_FEATURES]}\")\n",
    "        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
    "        print(f\"base_test[TARGET][mask]: {base_test[TARGET][mask]}\")\n",
    "        \n",
    "    # Make good column naming and add to all_preds DataFrame\n",
    "    temp_df = base_test[day_mask][['id',TARGET]]\n",
    "    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
    "    print(f\"temp_df: {temp_df}\")\n",
    "    \n",
    "    if 'id' in list(all_preds):\n",
    "        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
    "    else:\n",
    "        all_preds = temp_df.copy()\n",
    "        \n",
    "    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
    "                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
    "                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
    "    del temp_df\n",
    "    gc.collect()\n",
    "    \n",
    "all_preds = all_preds.reset_index(drop=True)\n",
    "print(f\"all_preds: {all_preds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export train/test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = pathlib.Path(os.path.abspath(os.curdir)).parent.parent\n",
    "# Reading competition sample submission and merging our predictions\n",
    "# As we have predictions only for \"_validation\" data we need to do fillna() for \"_evaluation\" items\n",
    "submission_df = read_csv_data(parent_dir, _SAMPLE_SUBMISSION_CSV_FILE)\n",
    "submission_ids_df = submission_df[[\"id\"]]\n",
    "display(submission_ids_df)\n",
    "\n",
    "# submission_df = pd.read_csv(ORIGINAL+_SAMPLE_SUBMISSION_CSV_FILE)[['id']]\n",
    "my_submission_df = submission_ids_df.merge(all_preds, on=['id'], how='left').fillna(0)\n",
    "\n",
    "_EXPORT_FILE_NAME = 'submission_v'+str(VER)+'_evaluation.csv'\n",
    "print(\"csv data export start\")\n",
    "my_submission_df.to_csv(os.path.sep.join([str(parent_dir), _OUTPUT_DIR, _EXPORT_FILE_NAME]), index=False)\n",
    "print('csv data export finished. Size:', grid_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course here is no magic at all.\n",
    "No \"Novel\" features and no brilliant ideas.\n",
    "We just carefully joined all\n",
    "our previous fe work and created a model.\n",
    "\n",
    "Also!\n",
    "In my opinion this strategy is a \"dead end\".\n",
    "Overfits a lot LB and with 1 final submission \n",
    "you have no option to risk.\n",
    "\n",
    "\n",
    "Improvement should come from:\n",
    "Loss function\n",
    "Data representation\n",
    "Stable CV\n",
    "Good features reduction strategy\n",
    "Predictions stabilization with NN\n",
    "Trend prediction\n",
    "Real zero sales detection/classification\n",
    "\n",
    "\n",
    "Good kernels references \n",
    "(the order is random and the list is not complete):\n",
    "https://www.kaggle.com/ragnar123/simple-lgbm-groupkfold-cv\n",
    "https://www.kaggle.com/jpmiller/grouping-items-by-stockout-pattern\n",
    "https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda\n",
    "https://www.kaggle.com/sibmike/m5-out-of-stock-feature\n",
    "https://www.kaggle.com/mayer79/m5-forecast-attack-of-the-data-table\n",
    "https://www.kaggle.com/yassinealouini/seq2seq\n",
    "https://www.kaggle.com/kailex/m5-forecaster-v2\n",
    "https://www.kaggle.com/aerdem4/m5-lofo-importance-on-gpu-via-rapids-xgboost\n",
    "\n",
    "\n",
    "Features were created in these kernels:\n",
    "# \n",
    "Mean encodings and PCA options\n",
    "https://www.kaggle.com/kyakovlev/m5-custom-features\n",
    "#\n",
    "Lags and rolling lags\n",
    "https://www.kaggle.com/kyakovlev/m5-lags-features\n",
    "#\n",
    "Base Grid and base features (calendar/price/etc)\n",
    "https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
    "\n",
    "\n",
    "Personal request\n",
    "Please don't upvote any ensemble and copypaste kernels\n",
    "The worst case is ensemble without any analyse.\n",
    "The best choice - just ignore it.\n",
    "I would like to see more kernels with interesting and original approaches.\n",
    "Don't feed copypasters with upvotes.\n",
    "\n",
    "It doesn't mean that you should not fork and improve others kernels\n",
    "but I would like to see params and code tuning based on some CV and analyse\n",
    "and not only on LB probing.\n",
    "Small changes could be shared in comments and authors can improve their kernel.\n",
    "\n",
    "Feel free to criticize this kernel as my knowlege is very limited\n",
    "and I can be wrong in code and descriptions. \n",
    "Thank you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
