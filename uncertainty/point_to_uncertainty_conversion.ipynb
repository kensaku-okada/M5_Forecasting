{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://www.kaggle.com/szmnkrisz97/point-to-uncertainty-different-ranges-per-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "import gc\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import psutil\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# custom import\n",
    "import scipy.stats  as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constant variables for helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_CORES: 12\n"
     ]
    }
   ],
   "source": [
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "print(f\"N_CORES: {N_CORES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function nicely diplaying a head of Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "def display(*dfs, head=True):\n",
    "    for df in dfs:\n",
    "        IPython.display.display(df.head() if head else df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function fixing random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    \"\"\"Sets seed to make all processes deterministic     # type: int\n",
    "    \n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function processing df in multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_df_in_multiprocess(func, t_split):\n",
    "    \"\"\"Process ds in Multiprocess\n",
    "    \n",
    "    \"\"\"\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"メモリ使用量を確認するためのシンプルな「メモリプロファイラ」\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "\n",
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    \"\"\"\n",
    "    dtypesを失わないための連結による結合\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  constant variables for data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the data from M5_accuracy\n",
    "_DATA_DIR = os.path.sep.join([\"data\", \"M5_Three_shades_of_Dark_Darker_magic\"])\n",
    "_OUTPUT_UNCERTAINTY_DIR = os.path.sep.join([\"data\", \"Point_to_uncertainty_different_ranges_per_level\"])\n",
    "_OUTPUT_DIR = _OUTPUT_UNCERTAINTY_DIR\n",
    "\n",
    "_CALENDAR_CSV_FILE = \"calendar.csv\"\n",
    "_SAMPLE_SUBMISSION_CSV_FILE = \"sample_submission.csv\"\n",
    "_SALES_TRAIN_VALIDATION_CSV_FILE = \"sales_train_validation.csv\"\n",
    "_SELL_PRICES_CSV_FILE = \"sell_prices.csv\"\n",
    "\n",
    "# _ACCURACY_DATA_DIR = os.path.sep.join([\"data\", \"M5_Three_shades_of_Dark_Darker_magic\", \"bk\", \"0.47353\"])\n",
    "_ACCURACY_RESULT_FILE = \"submission_v5_evaluation.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    reduce the memory usage of the given dataframe.\n",
    "    https://qiita.com/hiroyuki_kageyama/items/02865616811022f79754\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe\n",
    "        verbose: \n",
    "        \n",
    "    Returns:\n",
    "        df, whose memory usage is reduced.\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns: #columns毎に処理\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def read_csv_data(directory, file_name):\n",
    "    print('Reading files...')\n",
    "    df = pd.read_csv(os.path.sep.join([str(directory), _DATA_DIR, file_name]))\n",
    "    df = reduce_mem_usage(df)\n",
    "    print('{} has {} rows and {} columns'.format(file_name, df.shape[0], df.shape[1]))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent_dir: /Users/kensaku-okada-mac/jdsc/Documents/study meeting/ds/okada_presentation_202008/aws_sagemaker_forecast/M5_Forecasting\n",
      "Reading files...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /Users/kensaku-okada-mac/jdsc/Documents/study meeting/ds/okada_presentation_202008/aws_sagemaker_forecast/M5_Forecasting/data/M5_Three_shades_of_Dark_Darker_magic/sales_train_validation.csv does not exist: '/Users/kensaku-okada-mac/jdsc/Documents/study meeting/ds/okada_presentation_202008/aws_sagemaker_forecast/M5_Forecasting/data/M5_Three_shades_of_Dark_Darker_magic/sales_train_validation.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-82f9aaa7e070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"parent_dir: {parent_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_sales_train_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SALES_TRAIN_VALIDATION_CSV_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-d669c5183f8b>\u001b[0m in \u001b[0;36mread_csv_data\u001b[0;34m(directory, file_name)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reading files...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_DATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_mem_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} has {} rows and {} columns'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gurobi_with_python_3_7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gurobi_with_python_3_7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gurobi_with_python_3_7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gurobi_with_python_3_7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gurobi_with_python_3_7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /Users/kensaku-okada-mac/jdsc/Documents/study meeting/ds/okada_presentation_202008/aws_sagemaker_forecast/M5_Forecasting/data/M5_Three_shades_of_Dark_Darker_magic/sales_train_validation.csv does not exist: '/Users/kensaku-okada-mac/jdsc/Documents/study meeting/ds/okada_presentation_202008/aws_sagemaker_forecast/M5_Forecasting/data/M5_Three_shades_of_Dark_Darker_magic/sales_train_validation.csv'"
     ]
    }
   ],
   "source": [
    "parent_dir = pathlib.Path(os.path.abspath(os.curdir)).parent\n",
    "print(f\"parent_dir: {parent_dir}\")\n",
    "\n",
    "df_sales_train_validation = read_csv_data(parent_dir, _SALES_TRAIN_VALIDATION_CSV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import accuracy result\n",
    "df_accuracy_result = read_csv_data(parent_dir, _ACCURACY_RESULT_FILE)\n",
    "df_accuracy_result = reduce_mem_usage(df_accuracy_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_sample_submission = read_csv_data(parent_dir, _SAMPLE_SUBMISSION_CSV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(f\"df_sales_train_validation: {df_sales_train_validation.head()}\")\n",
    "# print(f\"df_sample_submission: {df_sample_submission.head()}\")\n",
    "# print(f\"df_sales_train_validation: {df_sales_train_validation}\")\n",
    "# print(f\"df_accuracy_result: {df_accuracy_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matches only \"*_validation\"\n",
    "sub = df_accuracy_result.merge(df_sales_train_validation[[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]], on = \"id\")\n",
    "sub[\"_all_\"] = \"Total\"\n",
    "print(f\"sub.shape: {sub.shape}\")\n",
    "print(f\"sub: {sub}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different ratios for different aggregation levels\n",
    "The higher the aggregation level, the more confident we are in the point prediction --> lower coef, relatively smaller range of quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = np.array([0.005,0.025,0.165,0.25, 0.5, 0.75, 0.835, 0.975, 0.995])\n",
    "\n",
    "def get_ratios(coef=0.15):\n",
    "    qs2 = np.log(qs/(1-qs))*coef\n",
    "#     print(f\"qs2: {qs2}\")\n",
    "    \n",
    "#     累積密度分布(cumulative distribution function)\n",
    "    ratios = stats.norm.cdf(qs2)\n",
    "#     print(f\" ratios: { ratios}\")\n",
    "    \n",
    "#     ratios[4] is 0.5\n",
    "    ratios /= ratios[4]\n",
    "#     print(f\" ratios: { ratios}\")\n",
    "    \n",
    "    ratios = pd.Series(ratios, index=qs)\n",
    "#     return ratios.round(3)\n",
    "    return ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coef between 0.05 and 0.24 is used, probably suboptimal values for now: <-グリッドサーチ手作りでベストなcoefの組み合わせ出せる？\n",
    "level_coef_dict = {\"id\": get_ratios(coef=0.3), \"item_id\": get_ratios(coef=0.15),\n",
    "                   \"dept_id\": get_ratios(coef=0.08), \"cat_id\": get_ratios(coef=0.07),\n",
    "                   \"store_id\": get_ratios(coef=0.08), \"state_id\": get_ratios(coef=0.07), \"_all_\": get_ratios(coef=0.05),\n",
    "                   (\"state_id\", \"item_id\"): get_ratios(coef=0.19),  (\"state_id\", \"dept_id\"): get_ratios(coef=0.1),\n",
    "                    (\"store_id\",\"dept_id\") : get_ratios(coef=0.11), (\"state_id\", \"cat_id\"): get_ratios(coef=0.08),\n",
    "                    (\"store_id\",\"cat_id\"): get_ratios(coef=0.1)\n",
    "                  }\n",
    "\n",
    "print(f\"level_coef_dict: {level_coef_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the the lowest level (i.e. \"id\") (30490 series), the smallest and biggest quantiles are 20% and 180% of the point prediction. For categories (\"cat_id\": 3 series), the model will be way more confident: the smallest quantile will be 71%, the biggest will be 129% of the point prediction.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_coefs(quantiles, level):\n",
    "#     特定レベルのquantile ratioを取得\n",
    "    ratios = level_coef_dict[level]\n",
    "\n",
    "#     各probablity intervalの閾値(quantiles)に対する倍率を取得\n",
    "    quantile_values = ratios.loc[quantiles].values\n",
    "#     print(f\"quantile_values: {quantile_values}\")\n",
    "#     print(f\"quantile_values[:, None]: {quantile_values[:, None]}\")\n",
    "    \n",
    "    return quantile_values\n",
    "\n",
    "def get_group_preds(pred, level, cols):\n",
    "#     levelごとにcols(各Fの値)を合計\n",
    "    df = pred.groupby(level)[cols].sum()\n",
    "    print(f\"df.shape: {df.shape}\")\n",
    "        \n",
    "    q = np.repeat(qs, len(df))    \n",
    "    print(f\"q.shape: {q.shape}\")\n",
    "    print(f\"q at get_group_preds: {q}\")\n",
    "    \n",
    "#     quantileの数は9\n",
    "    df = pd.concat([df]*9, axis=0, sort=False)\n",
    "    df.reset_index(inplace = True)\n",
    "    print(f\"amplified df: {df}\")\n",
    "\n",
    "#     accuracyにおける予測値の予測区間を、倍率を掛けることにより計算。[:, None]で行持ちを列持ちに変換\n",
    "    df[cols] *= quantile_coefs(q, level)[:, None]\n",
    "\n",
    "    if level != \"id\":\n",
    "#         uncertainty 用の提出ファイルに合わせるためのlabelの変更\n",
    "        df[\"id\"] = [f\"{lev}_X_{q:.3f}_validation\" for lev, q in zip(df[level].values, q)]\n",
    "    else:\n",
    "        df[\"id\"] = [f\"{lev.replace('_validation', '')}_{q:.3f}_validation\" for lev, q in zip(df[level].values, q)]\n",
    "    \n",
    "    \n",
    "    df = df[[\"id\"]+list(cols)]\n",
    "    return df\n",
    "\n",
    "def get_couple_group_preds(pred, level1, level2):\n",
    "    df = pred.groupby([level1, level2])[cols].sum()\n",
    "    print(f\"df.shape: {df.shape}\")\n",
    "\n",
    "    q = np.repeat(qs, len(df))\n",
    "    df = pd.concat([df]*9, axis=0, sort=False)\n",
    "    df.reset_index(inplace = True)\n",
    "        \n",
    "    df[cols] *= quantile_coefs(q, (level1, level2))[:, None]\n",
    "    df[\"id\"] = [f\"{lev1}_{lev2}_{q:.3f}_validation\" for lev1,lev2, q in \n",
    "                zip(df[level1].values,df[level2].values, q)]\n",
    "    df = df[[\"id\"]+list(cols)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\", \"_all_\"]\n",
    "couples = [(\"state_id\", \"item_id\"),  (\"state_id\", \"dept_id\"),(\"store_id\",\"dept_id\"), (\"state_id\", \"cat_id\"),(\"store_id\",\"cat_id\")]\n",
    "cols = [f\"F{i}\" for i in range(1, 29)]\n",
    "\n",
    "df = []\n",
    "for level in levels :\n",
    "    df.append(get_group_preds(sub, level, cols))\n",
    "    \n",
    "for level1,level2 in couples:\n",
    "    df.append(get_couple_group_preds(sub, level1, level2))\n",
    "\n",
    "print(f\"appended df: {df}\")\n",
    "\n",
    "# 縦方向に連結\n",
    "df = pd.concat(df, axis=0, sort=False)\n",
    "\n",
    "# inplace=Trueで元のオブジェクトを直接変更\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# validationとevaluation行　それぞれにコピーする(同じ値が格納される)\n",
    "df_submission = pd.concat([df,df] , axis=0, sort=False)\n",
    "df_submission.reset_index(drop=True, inplace=True)\n",
    "# 後半部分のid名を\"_evaluation\"に変更\n",
    "df_submission.loc[df_submission.index >= len(df_submission.index)//2, \"id\"] = df_submission.loc[df_submission.index >= len(df_submission.index)//2, \"id\"].str.replace(\n",
    "                                    \"_validation$\", \"_evaluation\")\n",
    "\n",
    "print(f\"df_submission: {df_submission}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# substitute the calculation result to the submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = pathlib.Path(os.path.abspath(os.curdir)).parent\n",
    "\n",
    "# Reading competition sample submission and merging our predictions\n",
    "submission_df = pd.read_csv(os.path.sep.join([str(parent_dir), _OUTPUT_UNCERTAINTY_DIR, _SAMPLE_SUBMISSION_CSV_FILE]))\n",
    "submission_df = reduce_mem_usage(submission_df)\n",
    "\n",
    "submission_ids_df = submission_df[[\"id\"]]\n",
    "\n",
    "# submission_df = pd.read_csv(ORIGINAL+_SAMPLE_SUBMISSION_CSV_FILE)[['id']]\n",
    "my_submission_df = submission_ids_df.merge(df, on=['id'], how='left').fillna(0)\n",
    "print(f\"my_submission_df:{my_submission_df}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export train/test result as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parent_dir = pathlib.Path(os.path.abspath(os.curdir)).parent\n",
    "\n",
    "VER = 5\n",
    "\n",
    "_EXPORT_FILE_NAME = 'submission_v'+str(VER)+'_validation.csv'\n",
    "print(\"csv data export start\")\n",
    "my_submission_df.to_csv(os.path.sep.join([str(parent_dir), _OUTPUT_DIR, _EXPORT_FILE_NAME]), index=False)\n",
    "print('csv data export finished. Size:', my_submission_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
